{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week 08 - Language Models (Part 2).ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "OE7fXh-OSJYF",
        "colab_type": "code",
        "outputId": "0f4971dc-efc2-4770-95d6-04019c50d782",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        }
      },
      "cell_type": "code",
      "source": [
        "!pip3 -qq install torch==0.4.1\n",
        "!pip -qq install torchtext==0.3.1\n",
        "!pip -qq install gensim==3.6.0\n",
        "!pip -qq install pyldavis==2.1.2\n",
        "!pip -qq install attrs==18.2.0\n",
        "!wget -qq --no-check-certificate 'https://drive.google.com/uc?export=download&id=1OIU9ICMebvZXJ0Grc2SLlMep3x9EkZtz' -O perashki.txt\n",
        "!wget -qq --no-check-certificate 'https://drive.google.com/uc?export=download&id=1v66uAEKL3KunyylYitNKggdl2gCeYgZZ' -O poroshki.txt\n",
        "!git clone https://github.com/UniversalDependencies/UD_Russian-SynTagRus.git\n",
        "!wget -qq https://raw.githubusercontent.com/DanAnastasyev/neuromorphy/master/neuromorphy/train/corpus_iterator.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tcmalloc: large alloc 1073750016 bytes == 0x58bde000 @  0x7f8f35c4a2a4 0x591a07 0x5b5d56 0x502e9a 0x506859 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x504c28 0x502540 0x502f3d 0x507641\n",
            "Cloning into 'UD_Russian-SynTagRus'...\n",
            "remote: Enumerating objects: 8, done.\u001b[K\n",
            "remote: Counting objects: 100% (8/8), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 284 (delta 2), reused 5 (delta 1), pack-reused 276\u001b[K\n",
            "Receiving objects: 100% (284/284), 154.19 MiB | 23.38 MiB/s, done.\n",
            "Resolving deltas: 100% (175/175), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uhvfH55PUJ8K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    from torch.cuda import FloatTensor, LongTensor\n",
        "    DEVICE = torch.device('cuda')\n",
        "else:\n",
        "    from torch import FloatTensor, LongTensor\n",
        "    DEVICE = torch.device('cpu')\n",
        "\n",
        "np.random.seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "txWqIO_74A4s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Word-Level Text Generation"
      ]
    },
    {
      "metadata": {
        "id": "KOD_3I7d4oDV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Сегодня занимаемся, в основном, тем, что генерируем *пирожки* и *порошки*.\n",
        "\n",
        "*(Данные без спросу скачаны с сайта http://poetory.ru)*\n",
        "\n",
        "Пирожки - это вот:"
      ]
    },
    {
      "metadata": {
        "id": "d2vMrlrRQpuJ",
        "colab_type": "code",
        "outputId": "41b398c2-bf89-45fe-d46d-0aca81733932",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "cell_type": "code",
      "source": [
        "!head perashki.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "старик вытягивает сети\r\n",
            "они пусты и лишь в конце\r\n",
            "записка рыба недоступна\r\n",
            "или вне действия сети\r\n",
            "\r\n",
            "олег адепт шизофрении\r\n",
            "шагает бодро из окна\r\n",
            "и жызнь летит перед глазами\r\n",
            "да не одна а сразу две\r\n",
            "\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0Lm0-PeG5Dh9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Порошки вот:"
      ]
    },
    {
      "metadata": {
        "id": "2-Jf88bxVTGj",
        "colab_type": "code",
        "outputId": "ec899c89-769e-4575-c9d9-712866933095",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "cell_type": "code",
      "source": [
        "!head poroshki.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "кто любит цоя кто покушать\r\n",
            "кто на рассвете пенье птиц\r\n",
            "а я люблю в коротких платьях\r\n",
            "физлиц\r\n",
            "\r\n",
            "твой монолог так гениален\r\n",
            "что я чуть не открыла дверь\r\n",
            "но станиславский тихо сверху\r\n",
            "не верь\r\n",
            "\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AgYh4FNP5FyX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Не перепутайте!\n",
        "\n",
        "Вообще, пирожок - это четверостишие, написанное четырехстопным ямбом по схеме 9-8-9-8. У порошка схема 9-8-9-2."
      ]
    },
    {
      "metadata": {
        "id": "bSBpLFRgGaXS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vowels = 'ёуеыаоэяию'\n",
        "\n",
        "odd_pattern = '-+-+-+-+-'\n",
        "even_pattern = '-+-+-+-+'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E2pFf5S_kXnQ",
        "colab_type": "code",
        "outputId": "241a1285-48ff-4c78-b1c2-52079533eeaa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "def count_vowels(word):\n",
        "    return len([letter for letter in word if letter in vowels])\n",
        "\n",
        "def get_pattern(token):\n",
        "    pattern = []\n",
        "    counter = 0\n",
        "    for word in token:\n",
        "        syl = count_vowels(word)\n",
        "        pattern.append(odd_pattern[counter : counter + syl] if counter + syl < len(odd_pattern) else '<pad>')\n",
        "        counter += syl\n",
        "    return pattern\n",
        "\n",
        "get_pattern(['покушать'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['-+-']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 167
        }
      ]
    },
    {
      "metadata": {
        "id": "Hl9BFoug519c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Считываем данные:"
      ]
    },
    {
      "metadata": {
        "id": "O3aFzzOQKLlD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def read_poem(path):\n",
        "    poem = []\n",
        "    poem_pattern = []\n",
        "    with open(path, encoding='utf8') as f:\n",
        "        for line in f:\n",
        "            line = line.rstrip()\n",
        "            pattern = get_pattern(line.split())\n",
        "            if len(line) == 0:\n",
        "                yield poem, poem_pattern\n",
        "                poem = []\n",
        "                poem_pattern = []\n",
        "                continue\n",
        "            \n",
        "            poem.extend(line.split() + ['\\\\n'])\n",
        "            poem_pattern.extend(pattern + ['\\\\n'])\n",
        "            \n",
        "perashki = list(read_poem('perashki.txt'))\n",
        "poroshki = list(read_poem('poroshki.txt'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tsVsxxpfQQ6i",
        "colab_type": "code",
        "outputId": "98adfe99-f92c-43ef-f291-3555d9728f55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "from torchtext.data import Field, Example, Dataset, BucketIterator\n",
        "\n",
        "text_field = Field(init_token='<s>', eos_token='</s>')\n",
        "pattern_field = Field(init_token='<s>', eos_token='</s>')\n",
        "        \n",
        "fields = [('text', text_field), ('pattern', pattern_field)]\n",
        "examples = [Example.fromlist([poem, pattern], fields) for poem, pattern in perashki]\n",
        "dataset = Dataset(examples, fields)\n",
        "\n",
        "text_field.build_vocab(dataset, min_freq=7)\n",
        "pattern_field.build_vocab(dataset, min_freq=7)\n",
        "\n",
        "print('Vocab size =', len(text_field.vocab))\n",
        "print('Pattern vocab size =', len(pattern_field.vocab))\n",
        "train_dataset, test_dataset = dataset.split(split_ratio=0.9)\n",
        "\n",
        "train_iter, test_iter = BucketIterator.splits(datasets=(train_dataset, test_dataset), batch_sizes=(64, 128), \n",
        "                                              shuffle=True, device=DEVICE, sort=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab size = 13070\n",
            "Pattern vocab size = 21\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "94XLc2pd4cJK",
        "colab_type": "code",
        "outputId": "84c3496d-a4a8-4688-eadc-fa24281890d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        }
      },
      "cell_type": "code",
      "source": [
        "pattern_field.vocab.itos"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<unk>',\n",
              " '<pad>',\n",
              " '<s>',\n",
              " '</s>',\n",
              " '\\\\n',\n",
              " '-',\n",
              " '-+',\n",
              " '+',\n",
              " '+-',\n",
              " '-+-',\n",
              " '+-+',\n",
              " '',\n",
              " '-+-+',\n",
              " '+-+-',\n",
              " '+-+-+',\n",
              " '-+-+-',\n",
              " '-+-+-+',\n",
              " '+-+-+-',\n",
              " '-+-+-+-',\n",
              " '-+-+-+-+',\n",
              " '+-+-+-+']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 170
        }
      ]
    },
    {
      "metadata": {
        "id": "xiRq1vbf55qN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Построим датасет для порошков:"
      ]
    },
    {
      "metadata": {
        "id": "ZOBgLAgVTrk1",
        "colab_type": "code",
        "outputId": "c48c2f85-4a27-4948-d87b-48b7564e637f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "from torchtext.data import Field, Example, Dataset, BucketIterator\n",
        "\n",
        "text_field = Field(init_token='<s>', eos_token='</s>')\n",
        "pattern_field = Field(init_token='<s>', eos_token='</s>')\n",
        "        \n",
        "fields = [('text', text_field), ('pattern', pattern_field)]\n",
        "examples = [Example.fromlist([poem, pattern], fields) for poem, pattern in poroshki]\n",
        "dataset = Dataset(examples, fields)\n",
        "\n",
        "text_field.build_vocab(dataset, min_freq=7)\n",
        "pattern_field.build_vocab(dataset, min_freq=7)\n",
        "\n",
        "print('Vocab size =', len(text_field.vocab))\n",
        "print('Pattern vocab size =', len(pattern_field.vocab))\n",
        "\n",
        "train_dataset, test_dataset = dataset.split(split_ratio=0.9)\n",
        "\n",
        "train_iter, test_iter = BucketIterator.splits(datasets=(train_dataset, test_dataset), batch_sizes=(32, 128), \n",
        "                                              shuffle=True, device=DEVICE, sort=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab size = 6298\n",
            "Pattern vocab size = 15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nw1Phd6hHGyK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch = next(iter(train_iter))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8FYJe2CA8GcY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Задание** Напишите класс языковой модели."
      ]
    },
    {
      "metadata": {
        "id": "x8ndCRZLl4ZZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class LMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, pattern_vocab_size=None, emb_dim=256, pattern_emb_dim=64, lstm_hidden_dim=256, num_layers=1):\n",
        "        super().__init__()\n",
        "\n",
        "        self._emb = nn.Embedding(vocab_size, emb_dim)\n",
        "        if pattern_vocab_size is not None:\n",
        "            self._pattern_emb = nn.Embedding(pattern_vocab_size, pattern_emb_dim)\n",
        "            self._rnn = nn.LSTM(input_size=emb_dim + pattern_emb_dim, hidden_size=lstm_hidden_dim)\n",
        "            self._pattern_out_layer = nn.Linear(lstm_hidden_dim, pattern_vocab_size)\n",
        "        else:\n",
        "            self._rnn = nn.LSTM(input_size=emb_dim, hidden_size=lstm_hidden_dim)\n",
        "        \n",
        "        self._out_layer = nn.Linear(lstm_hidden_dim, vocab_size)\n",
        "        \n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self, init_range=0.1):\n",
        "        self._emb.weight.data.uniform_(-init_range, init_range)\n",
        "        self._out_layer.bias.data.zero_()\n",
        "#         if pattern_vocab_size is not None:\n",
        "#             self._out_layer.weight.data.uniform_(-init_range, init_range)\n",
        "#         else:\n",
        "        self._out_layer.weight = self._emb.weight\n",
        "        \n",
        "\n",
        "    def forward(self, inputs, pattern_inputs=None, hidden=None):\n",
        "        embeds = self._emb(inputs)\n",
        "        \n",
        "        if pattern_inputs is not None:\n",
        "            pattern_embeds = self._pattern_emb(pattern_inputs)\n",
        "            embeds = torch.cat((embeds, pattern_embeds), 2)\n",
        "            \n",
        "        outputs, hidden = self._rnn(embeds, hidden)\n",
        "        \n",
        "        text_outputs = self._out_layer(outputs)\n",
        "        if pattern_inputs is not None:\n",
        "            pattern_outputs = self._pattern_out_layer(outputs)\n",
        "            \n",
        "            return text_outputs, pattern_outputs, hidden\n",
        "        \n",
        "        return text_outputs, hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "dcf5ed42-708e-4e29-f628-0dd73d7a876e",
        "id": "x-S9TvxbyK0l",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "model = LMModel(vocab_size=len(train_iter.dataset.fields['text'].vocab), pattern_vocab_size=len(train_iter.dataset.fields['pattern'].vocab)).to(DEVICE)\n",
        "\n",
        "model(batch.text, batch.pattern)[0].shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([29, 64, 13070])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 151
        }
      ]
    },
    {
      "metadata": {
        "id": "Rsh3_eR08PqQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Задание** Добавьте подсчет потерей с маскингом паддингов."
      ]
    },
    {
      "metadata": {
        "id": "_E2JxfRuphch",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import math\n",
        "from tqdm import tqdm\n",
        "tqdm.get_lock().locks = []\n",
        "\n",
        "\n",
        "def do_epoch(model, criterion, data_iter, unk_idx, pad_idx, use_patterns=False, optimizer=None, name=None):\n",
        "    epoch_loss = 0\n",
        "    \n",
        "    is_train = not optimizer is None\n",
        "    name = name or ''\n",
        "    model.train(is_train)\n",
        "    \n",
        "    batches_count = len(data_iter)\n",
        "    \n",
        "    with torch.autograd.set_grad_enabled(is_train):\n",
        "#         with tqdm(total=batches_count) as progress_bar:\n",
        "            for i, batch in enumerate(data_iter):    \n",
        "                labels = batch.text[1:, :]\n",
        "                labels = labels.view(-1)\n",
        "                \n",
        "                if use_patterns:\n",
        "                    pattern_labels = batch.pattern[1:, :]\n",
        "                    pattern_labels = pattern_labels.view(-1)\n",
        "                \n",
        "                    logits, pattern_logits, _ = model(batch.text, batch.pattern)\n",
        "                    \n",
        "                    pattern_logits = pattern_logits[:-1, :, :]\n",
        "                    pattern_logits = pattern_logits.view(-1, pattern_logits.shape[-1])\n",
        "                    \n",
        "                    mask = ((pattern_labels != 0) * (pattern_labels != 1)).float()  # fixme\n",
        "                    \n",
        "                    pattern_loss = torch.sum(criterion(pattern_logits, pattern_labels.view(-1)) * mask) / torch.sum(mask)\n",
        "                else:\n",
        "                    logits, _ = model(batch.text)\n",
        "                logits = logits[:-1, :, :]\n",
        "                logits = logits.view(-1, logits.shape[-1])\n",
        "                \n",
        "                mask = ((labels != pad_idx) * (labels != unk_idx)).float()\n",
        "                \n",
        "                loss = torch.sum(criterion(logits, labels.view(-1)) * mask) / torch.sum(mask)\n",
        "                if use_patterns:\n",
        "                    loss = (loss + pattern_loss) / 2\n",
        "                \n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "                if optimizer:\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    nn.utils.clip_grad_norm_(model.parameters(), 1.)\n",
        "                    optimizer.step()\n",
        "\n",
        "#                 progress_bar.update()\n",
        "#                 progress_bar.set_description('{:>5s} Loss = {:.5f}, PPX = {:.2f}'.format(name, loss.item(), \n",
        "#                                                                                          math.exp(loss.item())))\n",
        "                \n",
        "#             progress_bar.set_description('{:>5s} Loss = {:.5f}, PPX = {:.2f}'.format(\n",
        "#                 name, epoch_loss / batches_count, math.exp(epoch_loss / batches_count))\n",
        "#             )\n",
        "#             progress_bar.refresh()\n",
        "            print('{:>5s} Loss = {:.5f}, PPX = {:.2f}'.format(name, epoch_loss / batches_count, math.exp(epoch_loss / batches_count)))\n",
        "                    \n",
        "    return epoch_loss / batches_count\n",
        "\n",
        "\n",
        "def fit(model, criterion, optimizer, train_iter, use_patterns=False, epochs_count=1, unk_idx=0, pad_idx=1, val_iter=None):\n",
        "    best_val_loss = None\n",
        "    for epoch in range(epochs_count):\n",
        "        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n",
        "        train_loss = do_epoch(model, criterion, train_iter, unk_idx, pad_idx, use_patterns, optimizer, name_prefix + 'Train:')\n",
        "        \n",
        "        if not val_iter is None:\n",
        "            val_loss = do_epoch(model, criterion, val_iter, unk_idx, pad_idx, use_patterns, None, name_prefix + '  Val:')\n",
        "            \n",
        "            if best_val_loss and val_loss > best_val_loss:\n",
        "                optimizer.param_groups[0]['lr'] /= 4.\n",
        "                print('Optimizer lr = {:g}'.format(optimizer.param_groups[0]['lr']))\n",
        "            else:\n",
        "                best_val_loss = val_loss\n",
        "        print()\n",
        "        generate(model, use_patterns)\n",
        "        print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ufpoSwQ-8bcN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Задание** Напишите функцию-генератор для модели."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "BYoHY1se2bhB",
        "outputId": "cf336b42-5eb5-4297-c1b0-858b2a5df290",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "cell_type": "code",
      "source": [
        "def sample(probs, temp):\n",
        "    probs = F.log_softmax(probs.squeeze(), dim=0)\n",
        "    probs = (probs / temp).exp()\n",
        "    probs /= probs.sum()\n",
        "    probs = probs.cpu().numpy()\n",
        "\n",
        "    return np.random.choice(np.arange(len(probs)), p=probs)\n",
        "\n",
        "\n",
        "def generate(model, use_patterns=False, temp=0.6):\n",
        "    model.eval()\n",
        "    with torch.no_grad():        \n",
        "        prev_token = train_iter.dataset.fields['text'].vocab.stoi['<s>']\n",
        "        end_token = train_iter.dataset.fields['text'].vocab.stoi['</s>']\n",
        "        if use_patterns:\n",
        "            prev_pattern_token = train_iter.dataset.fields['pattern'].vocab.stoi['<s>']\n",
        "        \n",
        "        new_line_token = train_iter.dataset.fields['text'].vocab.stoi['\\\\n']\n",
        "        \n",
        "        hidden = None\n",
        "        for _ in range(150):\n",
        "            if use_patterns:\n",
        "                probs, pattern_probs, hidden = model(LongTensor([[prev_token]]), LongTensor([[prev_pattern_token]]), hidden)\n",
        "            else:\n",
        "                probs, hidden = model(LongTensor([[prev_token]]), None, hidden)\n",
        "            prev_token = sample(probs[-1], temp)\n",
        "            \n",
        "            if use_patterns:\n",
        "#                 word = train_dataset.fields['text'].vocab.itos[prev_token]\n",
        "#                 pattern = get_pattern(word)\n",
        "#                 prev_pattern_token = train_iter.dataset.fields['pattern'].vocab.stoi[pattern]\n",
        "                \n",
        "                prev_pattern_token = sample(pattern_probs[-1], temp)\n",
        "                pattern = train_iter.dataset.fields['pattern'].vocab.itos[prev_pattern_token]\n",
        "            \n",
        "            if prev_token == end_token:\n",
        "                return\n",
        "            if prev_token != new_line_token and use_patterns:\n",
        "                print(train_dataset.fields['text'].vocab.itos[prev_token] + \" (\" + train_iter.dataset.fields['pattern'].vocab.itos[prev_pattern_token] + \")\", end=' ')\n",
        "            elif prev_token != new_line_token:\n",
        "                print(train_dataset.fields['text'].vocab.itos[prev_token], end=' ')\n",
        "            else:\n",
        "                print()\n",
        "            \n",
        "                \n",
        "generate(model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "я вас люблю сказал аркадий \n",
            "и в эти несколько часов \n",
            "и вдруг один из них с тобою \n",
            "мне в сердце нечего любовь \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5X2kYDU_rCjP",
        "colab_type": "code",
        "outputId": "7f36eab9-bf39-4449-bfa8-972da8fb9964",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3071
        }
      },
      "cell_type": "code",
      "source": [
        "model = LMModel(vocab_size=len(train_iter.dataset.fields['text'].vocab)).to(DEVICE) # pattern_vocab_size=len(train_iter.dataset.fields['pattern'].vocab)\n",
        "\n",
        "pad_idx = train_iter.dataset.fields['text'].vocab.stoi['<pad>']\n",
        "unk_idx = train_iter.dataset.fields['text'].vocab.stoi['<unk>']\n",
        "criterion = nn.CrossEntropyLoss(reduction='none').to(DEVICE)\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=20., weight_decay=1e-6)\n",
        "\n",
        "fit(model, criterion, optimizer, train_iter, use_patterns=False, epochs_count=300, unk_idx=unk_idx, pad_idx=pad_idx, val_iter=test_iter)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 / 300] Train: Loss = 5.60144, PPX = 270.82\n",
            "[1 / 300]   Val: Loss = 5.04384, PPX = 155.06\n",
            "\n",
            "олег на вспомнив в руках и в поле \n",
            "сказал я в квартиру и в груди \n",
            "и по то что я в гробу на поле \n",
            "не я не знаю как не на этом \n",
            "\n",
            "[2 / 300] Train: Loss = 4.88843, PPX = 132.75\n",
            "[2 / 300]   Val: Loss = 4.76736, PPX = 117.61\n",
            "\n",
            "конец когда ты спросила мне \n",
            "что я не видел как я так \n",
            "а ты уже тебя и я \n",
            "я всё же я люблю люблю \n",
            "\n",
            "[3 / 300] Train: Loss = 4.59648, PPX = 99.13\n",
            "[3 / 300]   Val: Loss = 4.57564, PPX = 97.09\n",
            "\n",
            "нет ничего не видел смысла \n",
            "я не могу я помню это \n",
            "что ты не будет никогда \n",
            "и не выходит за тобой \n",
            "\n",
            "[4 / 300] Train: Loss = 4.41046, PPX = 82.31\n",
            "[4 / 300]   Val: Loss = 4.48467, PPX = 88.65\n",
            "\n",
            "я не люблю тебя я умер \n",
            "и что то я не человек \n",
            "а ты мне нравится что время \n",
            "не будет так не я не ждал \n",
            "\n",
            "[5 / 300] Train: Loss = 4.26175, PPX = 70.93\n",
            "[5 / 300]   Val: Loss = 4.43870, PPX = 84.67\n",
            "\n",
            "в моей квартире все шаинский \n",
            "и смотрит в рот и говорит \n",
            "что он в метро пришли по плану \n",
            "и без цветов не отвечал \n",
            "\n",
            "[6 / 300] Train: Loss = 4.12856, PPX = 62.09\n",
            "[6 / 300]   Val: Loss = 4.38318, PPX = 80.09\n",
            "\n",
            "я в детстве с детства был на даче \n",
            "и не пускают в этот раз \n",
            "но я на кухне вместо миром \n",
            "и я храню тебе назад \n",
            "\n",
            "[7 / 300] Train: Loss = 4.04326, PPX = 57.01\n",
            "[7 / 300]   Val: Loss = 4.38816, PPX = 80.49\n",
            "Optimizer lr = 5\n",
            "\n",
            "а есть ли жизнь как мы не пишем \n",
            "но где то тихо не привык \n",
            "а человек на красный полке \n",
            "и у меня с лица земли \n",
            "\n",
            "[8 / 300] Train: Loss = 3.76237, PPX = 43.05\n",
            "[8 / 300]   Val: Loss = 4.31904, PPX = 75.12\n",
            "\n",
            "я в детстве знал и в этой жизни \n",
            "не может быть на чем смотреть \n",
            "и что за каждой все забыли \n",
            "и даже не вместо ничего \n",
            "\n",
            "[9 / 300] Train: Loss = 3.69020, PPX = 40.05\n",
            "[9 / 300]   Val: Loss = 4.31579, PPX = 74.87\n",
            "\n",
            "я так люблю тебя родная \n",
            "что я не знаю что меня \n",
            "а я не знаю что я умер \n",
            "но я не пью и не могу \n",
            "\n",
            "[10 / 300] Train: Loss = 3.63673, PPX = 37.97\n",
            "[10 / 300]   Val: Loss = 4.31861, PPX = 75.08\n",
            "Optimizer lr = 1.25\n",
            "\n",
            "семён ильич садится в полночь \n",
            "и достает его окно \n",
            "и говорят что в нас хватает \n",
            "и есть в деревне у него \n",
            "\n",
            "[11 / 300] Train: Loss = 3.53771, PPX = 34.39\n",
            "[11 / 300]   Val: Loss = 4.31575, PPX = 74.87\n",
            "\n",
            "мы все не так уж не выходит \n",
            "с олегом в жизни без зонта \n",
            "то есть и даже чтото смотрит \n",
            "и на поверхность простынях \n",
            "\n",
            "[12 / 300] Train: Loss = 3.51727, PPX = 33.69\n",
            "[12 / 300]   Val: Loss = 4.31914, PPX = 75.12\n",
            "Optimizer lr = 0.3125\n",
            "\n",
            "и я не вас не буду плакать \n",
            "не делай мне на этот раз \n",
            "не в смысле смысле что я умер \n",
            "а в смысле что в нём дома нет \n",
            "\n",
            "[13 / 300] Train: Loss = 3.48681, PPX = 32.68\n",
            "[13 / 300]   Val: Loss = 4.31769, PPX = 75.01\n",
            "Optimizer lr = 0.078125\n",
            "\n",
            "я умер но не шостакович \n",
            "а просто может я не сплю \n",
            "а смерть не надо мне не надо \n",
            "я только не могу заснуть \n",
            "\n",
            "[14 / 300] Train: Loss = 3.47787, PPX = 32.39\n",
            "[14 / 300]   Val: Loss = 4.31664, PPX = 74.94\n",
            "Optimizer lr = 0.0195312\n",
            "\n",
            "когда губу с тобой в искусстве \n",
            "он был и не попал в театр \n",
            "а я с тобой уже не в силах \n",
            "ни леса глаз и не алкаш \n",
            "\n",
            "[15 / 300] Train: Loss = 3.47527, PPX = 32.31\n",
            "[15 / 300]   Val: Loss = 4.31754, PPX = 75.00\n",
            "Optimizer lr = 0.00488281\n",
            "\n",
            "когда я стану взрослым плакать \n",
            "то вдруг как я тебя люблю \n",
            "как будто я её снимаю \n",
            "и в эти волосы всё то \n",
            "\n",
            "[16 / 300] Train: Loss = 3.47474, PPX = 32.29\n",
            "[16 / 300]   Val: Loss = 4.31824, PPX = 75.06\n",
            "Optimizer lr = 0.0012207\n",
            "\n",
            "ко мне подходит мой друг друга \n",
            "а ты в двенадцать раз хочу \n",
            "что с ним уже давно не может \n",
            "а скоро буду отвечать \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-154-2bf022f3b53e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_patterns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munk_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munk_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpad_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-153-15e41b8fd77b>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(model, criterion, optimizer, train_iter, use_patterns, epochs_count, unk_idx, pad_idx, val_iter)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mname_prefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'[{} / {}] '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdo_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munk_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_patterns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname_prefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'Train:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mval_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-153-15e41b8fd77b>\u001b[0m in \u001b[0;36mdo_epoch\u001b[0;34m(model, criterion, data_iter, unk_idx, pad_idx, use_patterns, optimizer, name)\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m                     \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "Oyq3HtALUBuU",
        "colab_type": "code",
        "outputId": "09239783-016c-44f2-bf07-8ca3a4a09a51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 7014
        }
      },
      "cell_type": "code",
      "source": [
        "model_with_patterns = LMModel(vocab_size=len(train_iter.dataset.fields['text'].vocab), pattern_vocab_size=len(train_iter.dataset.fields['pattern'].vocab)).to(DEVICE) # \n",
        "\n",
        "pad_idx = train_iter.dataset.fields['text'].vocab.stoi['<pad>']\n",
        "unk_idx = train_iter.dataset.fields['text'].vocab.stoi['<unk>']\n",
        "criterion = nn.CrossEntropyLoss(reduction='none').to(DEVICE)\n",
        "\n",
        "optimizer = optim.SGD(model_with_patterns.parameters(), lr=20., weight_decay=1e-6)\n",
        "\n",
        "fit(model_with_patterns, criterion, optimizer, train_iter, use_patterns=True, epochs_count=300, unk_idx=unk_idx, pad_idx=pad_idx, val_iter=test_iter)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 / 300] Train: Loss = 6.81910, PPX = 915.16\n",
            "[1 / 300]   Val: Loss = 5.89147, PPX = 361.94\n",
            "\n",
            "олег я с вами \n",
            "на твоих \n",
            "очках мне с ними \n",
            "я тут день с ним не не не же что есть не то же же всё что же с ним же же и же же ни не я как что же же же же день как пор не же меня \n",
            "я со думал \n",
            "из не с бутылкой \n",
            "с собой \n",
            "я мне на на вот же с тобою \n",
            "я чем нибудь \n",
            "\n",
            "[2 / 300] Train: Loss = 6.85604, PPX = 949.60\n",
            "[2 / 300]   Val: Loss = 6.85798, PPX = 951.44\n",
            "Optimizer lr = 5\n",
            "\n",
            "когда у нас с тобою \n",
            "тот на столе \n",
            "я с горочки \n",
            "\n",
            "[3 / 300] Train: Loss = 5.26675, PPX = 193.79\n",
            "[3 / 300]   Val: Loss = 5.21210, PPX = 183.48\n",
            "\n",
            "вот вот я жена за них я наши не было \n",
            "а я в том же там не будет в дневнике \n",
            "\n",
            "[4 / 300] Train: Loss = 5.13893, PPX = 170.53\n",
            "[4 / 300]   Val: Loss = 5.10208, PPX = 164.36\n",
            "\n",
            "сегодня я не любишь \n",
            "а я не надо вышла \n",
            "так тот мне в неделю \n",
            "и молвил нет не надо \n",
            "а если больше на работу \n",
            "\n",
            "[5 / 300] Train: Loss = 5.02604, PPX = 152.33\n",
            "[5 / 300]   Val: Loss = 4.99745, PPX = 148.04\n",
            "\n",
            "в костюме все и в нём то это \n",
            "а мы не надо снег и это \n",
            "\n",
            "[6 / 300] Train: Loss = 4.93531, PPX = 139.12\n",
            "[6 / 300]   Val: Loss = 4.96000, PPX = 142.59\n",
            "\n",
            "у нас в костюме рублей \n",
            "а я за бога всё не в смысле \n",
            "как хорошо что ты не так \n",
            "а просто что нибудь на этом \n",
            "\n",
            "[7 / 300] Train: Loss = 4.86748, PPX = 129.99\n",
            "[7 / 300]   Val: Loss = 4.89551, PPX = 133.69\n",
            "\n",
            "я в доме утренней с детства \n",
            "что я люблю не может что \n",
            "а я не знал что я \n",
            "а после ни его не так \n",
            "\n",
            "[8 / 300] Train: Loss = 4.80625, PPX = 122.27\n",
            "[8 / 300]   Val: Loss = 4.82807, PPX = 124.97\n",
            "\n",
            "как хорошо что папа дома \n",
            "а я тебя и смерть готов \n",
            "а в жизни и не в смысле \n",
            "и я такая как то я \n",
            "а в смысле он же как же \n",
            "и спит и в мире надолго \n",
            "\n",
            "[9 / 300] Train: Loss = 4.73138, PPX = 113.45\n",
            "[9 / 300]   Val: Loss = 4.74618, PPX = 115.14\n",
            "\n",
            "как раньше был и я и стал \n",
            "а это что то в ней не в шутку \n",
            "что ты меня не ешь как я \n",
            "а в мире в год и в ванну темноте \n",
            "\n",
            "[10 / 300] Train: Loss = 4.65510, PPX = 105.12\n",
            "[10 / 300]   Val: Loss = 4.70986, PPX = 111.04\n",
            "\n",
            "на кладбище а в нём олег \n",
            "а в нём а дальше я в постель \n",
            "а вы что я с тобой в постель \n",
            "\n",
            "[11 / 300] Train: Loss = 4.59986, PPX = 99.47\n",
            "[11 / 300]   Val: Loss = 4.66838, PPX = 106.53\n",
            "\n",
            "я не люблю тебя не видит \n",
            "а если был не в силах а \n",
            "а вот в порядке в чём то время \n",
            "а в смысле что там где то в жизни \n",
            "а я его в ладошку на \n",
            "\n",
            "[12 / 300] Train: Loss = 4.54910, PPX = 94.55\n",
            "[12 / 300]   Val: Loss = 4.63491, PPX = 103.02\n",
            "\n",
            "когда я думал мне не так же \n",
            "что мы была не в смысле \n",
            "а я в одном сегодня не \n",
            "я буду даже не люблю \n",
            "\n",
            "[13 / 300] Train: Loss = 4.51077, PPX = 90.99\n",
            "[13 / 300]   Val: Loss = 4.60582, PPX = 100.07\n",
            "\n",
            "вот так в квартире я реально \n",
            "для вечер мере и не в такт \n",
            "а ты на что я в нём же \n",
            "как ты меня не так же нет \n",
            "\n",
            "[14 / 300] Train: Loss = 4.46232, PPX = 86.69\n",
            "[14 / 300]   Val: Loss = 4.57435, PPX = 96.96\n",
            "\n",
            "олег в тюрьме и все на море \n",
            "а где то что то в этом раз \n",
            "а может ты куда ты в жизни \n",
            "а ты меня за хлебом свет \n",
            "\n",
            "[15 / 300] Train: Loss = 4.41911, PPX = 83.02\n",
            "[15 / 300]   Val: Loss = 4.54605, PPX = 94.26\n",
            "\n",
            "я не могу тебя не любит \n",
            "и я могу тебя люблю \n",
            "а я хотел все это не взял \n",
            "а я пока не тот же любишь \n",
            "и в школу к одному как дождь \n",
            "\n",
            "[16 / 300] Train: Loss = 4.37429, PPX = 79.38\n",
            "[16 / 300]   Val: Loss = 4.51137, PPX = 91.05\n",
            "\n",
            "я не могу тебя о жизни \n",
            "а ты а ты не будешь ни \n",
            "а вы что создал мне в огонь \n",
            "\n",
            "[17 / 300] Train: Loss = 4.32891, PPX = 75.86\n",
            "[17 / 300]   Val: Loss = 4.48929, PPX = 89.06\n",
            "\n",
            "я напишу из жопы в жопу \n",
            "что я не буду был в глаза \n",
            "а я боюсь в блокнот и в сердце \n",
            "когда ты в москву об этом \n",
            "а я в ответ то в банке месте \n",
            "а ты не в силах не заметил \n",
            "я в шахматы и в кипяток \n",
            "\n",
            "[18 / 300] Train: Loss = 4.28607, PPX = 72.68\n",
            "[18 / 300]   Val: Loss = 4.45944, PPX = 86.44\n",
            "\n",
            "я я не видел я и знаю \n",
            "что я пей раз и я \n",
            "как будто я не знаю что в нём \n",
            "как в детстве на полу а в рай \n",
            "\n",
            "[19 / 300] Train: Loss = 4.24510, PPX = 69.76\n",
            "[19 / 300]   Val: Loss = 4.43924, PPX = 84.71\n",
            "\n",
            "в метро приехал в обморок \n",
            "а в нём олег и в нём не скучно \n",
            "я в лодку в этот раз и ты \n",
            "что я тебя люблю люблю то \n",
            "а я вам тоже не могу \n",
            "\n",
            "[20 / 300] Train: Loss = 4.20661, PPX = 67.13\n",
            "[20 / 300]   Val: Loss = 4.41545, PPX = 82.72\n",
            "\n",
            "когда ты в школе был не буду \n",
            "а я не знал про да я сплю \n",
            "как мы с тобой и мы ходили \n",
            "ведь я бы не люблю тебя \n",
            "\n",
            "[21 / 300] Train: Loss = 4.16845, PPX = 64.62\n",
            "[21 / 300]   Val: Loss = 4.40104, PPX = 81.54\n",
            "\n",
            "олег на трассе с николаем \n",
            "а мне в окно и как в метро \n",
            "ты мне на что вы мне грустить приедет \n",
            "на этот день еще один \n",
            "\n",
            "[22 / 300] Train: Loss = 4.13213, PPX = 62.31\n",
            "[22 / 300]   Val: Loss = 4.38500, PPX = 80.24\n",
            "\n",
            "а ты все больше мне на свете \n",
            "все нет не буду ничего \n",
            "что борщ не надо было дальше \n",
            "ты не меня никто не знал \n",
            "\n",
            "[23 / 300] Train: Loss = 4.09582, PPX = 60.09\n",
            "[23 / 300]   Val: Loss = 4.37045, PPX = 79.08\n",
            "\n",
            "геннадий вышел из барака \n",
            "как будто в дверь олег лежит \n",
            "все время тихо чтоб на дело \n",
            "все хорошо что будет два \n",
            "\n",
            "[24 / 300] Train: Loss = 4.06059, PPX = 58.01\n",
            "[24 / 300]   Val: Loss = 4.35234, PPX = 77.66\n",
            "\n",
            "в одном над городом и умер \n",
            "а у меня такая смерть \n",
            "пока олег не хочет утром \n",
            "ты как то хорошо что он \n",
            "\n",
            "[25 / 300] Train: Loss = 4.02571, PPX = 56.02\n",
            "[25 / 300]   Val: Loss = 4.34493, PPX = 77.09\n",
            "\n",
            "а тут тебя теперь ты в коме \n",
            "ты как бы по домам смотреть \n",
            "то в этом месте не бывает \n",
            "как будто бы в последний раз \n",
            "\n",
            "[26 / 300] Train: Loss = 3.99113, PPX = 54.12\n",
            "[26 / 300]   Val: Loss = 4.33152, PPX = 76.06\n",
            "\n",
            "я в детстве был не так уж долго \n",
            "я это буду за чего \n",
            "а мне бы женщина в висок \n",
            "но я не знал что я скажу \n",
            "\n",
            "[27 / 300] Train: Loss = 3.95765, PPX = 52.33\n",
            "[27 / 300]   Val: Loss = 4.32157, PPX = 75.31\n",
            "\n",
            "я не хочу вам в жизни жить в чём то \n",
            "так долго жить и без меня \n",
            "я знаю это был не очень \n",
            "как будто бы не напишу \n",
            "\n",
            "[28 / 300] Train: Loss = 3.92461, PPX = 50.63\n",
            "[28 / 300]   Val: Loss = 4.31130, PPX = 74.54\n",
            "\n",
            "я земленавт и вижу в бога \n",
            "когда я на ушко мне \n",
            "то я ответил что вы мне что \n",
            "я буду в зеркало люблю \n",
            "\n",
            "[29 / 300] Train: Loss = 3.89159, PPX = 48.99\n",
            "[29 / 300]   Val: Loss = 4.30345, PPX = 73.95\n",
            "\n",
            "когда умру то не в оксану \n",
            "я вижу как то я люблю \n",
            "себе нетрезвый выпал \n",
            "я не люблю тебя я вас \n",
            "\n",
            "[30 / 300] Train: Loss = 3.85980, PPX = 47.46\n",
            "[30 / 300]   Val: Loss = 4.29673, PPX = 73.46\n",
            "\n",
            "сегодня будет в понедельник \n",
            "то видно солнца пирожки \n",
            "я вижу в городе в сети \n",
            "с тех пор как умер вы ж послал \n",
            "\n",
            "[31 / 300] Train: Loss = 3.82766, PPX = 45.95\n",
            "[31 / 300]   Val: Loss = 4.29047, PPX = 73.00\n",
            "\n",
            "я помню как я вижу море \n",
            "я не люблю тебя люблю \n",
            "а я в руках и все такое \n",
            "что я не сплю и не хочу \n",
            "\n",
            "[32 / 300] Train: Loss = 3.79584, PPX = 44.52\n",
            "[32 / 300]   Val: Loss = 4.28678, PPX = 72.73\n",
            "\n",
            "а я в гостях у самой ночи \n",
            "я буду всех и вас люблю \n",
            "там хорошо я ненавижу \n",
            "как будто ты не заслужил \n",
            "\n",
            "[33 / 300] Train: Loss = 3.78036, PPX = 43.83\n",
            "[33 / 300]   Val: Loss = 4.28087, PPX = 72.30\n",
            "\n",
            "я пью пока я понимаю \n",
            "меня ты в меня то в ад \n",
            "туда где сердце у окошка \n",
            "для сна у всех своих ушей \n",
            "\n",
            "[34 / 300] Train: Loss = 3.73559, PPX = 41.91\n",
            "[34 / 300]   Val: Loss = 4.27567, PPX = 71.93\n",
            "\n",
            "у всех есть город с николаем \n",
            "все лето ходит по воде \n",
            "пока что не на самом деле \n",
            "то есть у нас у вас не я \n",
            "\n",
            "[35 / 300] Train: Loss = 3.70491, PPX = 40.65\n",
            "[35 / 300]   Val: Loss = 4.28024, PPX = 72.26\n",
            "Optimizer lr = 1.25\n",
            "\n",
            "когда не стало быть счастливым \n",
            "то хоть в один ещё раз раз \n",
            "на день рожденья тридцать восемь \n",
            "то на него совсем не все \n",
            "\n",
            "[36 / 300] Train: Loss = 3.61850, PPX = 37.28\n",
            "[36 / 300]   Val: Loss = 4.26298, PPX = 71.02\n",
            "\n",
            "я не люблю тебя аркадий \n",
            "там где то в этом все когда \n",
            "то это значит что не знаю \n",
            "что ты меня не в силах так \n",
            "\n",
            "[37 / 300] Train: Loss = 3.60382, PPX = 36.74\n",
            "[37 / 300]   Val: Loss = 4.26215, PPX = 70.96\n",
            "\n",
            "на самом деле всё иначе \n",
            "что я не знаю ничего \n",
            "я буду пить с тобой к груди \n",
            "я с детства помню помню как в нем \n",
            "меня в душе сказал я там \n",
            "\n",
            "[38 / 300] Train: Loss = 3.59399, PPX = 36.38\n",
            "[38 / 300]   Val: Loss = 4.26394, PPX = 71.09\n",
            "Optimizer lr = 0.3125\n",
            "\n",
            "я как поэт спросил олега \n",
            "то он на глубине похож \n",
            "то в ней как в небо николая \n",
            "ещё раз в наш дом приходит \n",
            "пока что у них там снег \n",
            "\n",
            "[39 / 300] Train: Loss = 3.56883, PPX = 35.47\n",
            "[39 / 300]   Val: Loss = 4.26131, PPX = 70.90\n",
            "\n",
            "я не могу как в детстве дома \n",
            "то не успею никогда \n",
            "я был бы знал что это значит \n",
            "что я за все ж любви нибудь \n",
            "\n",
            "[40 / 300] Train: Loss = 3.56528, PPX = 35.35\n",
            "[40 / 300]   Val: Loss = 4.25801, PPX = 70.67\n",
            "\n",
            "психолог был на встречу жизни \n",
            "на кухню вышел и в обед \n",
            "олег в один один старушка \n",
            "олег в мой дом был и в очках \n",
            "\n",
            "[41 / 300] Train: Loss = 3.56260, PPX = 35.25\n",
            "[41 / 300]   Val: Loss = 4.26261, PPX = 71.00\n",
            "Optimizer lr = 0.078125\n",
            "\n",
            "а в небе как там в центре центре \n",
            "а в нём не хватит и в людей \n",
            "олег почувствовал а там \n",
            "с ним есть он там и плохо есть \n",
            "\n",
            "[42 / 300] Train: Loss = 3.55558, PPX = 35.01\n",
            "[42 / 300]   Val: Loss = 4.26234, PPX = 70.98\n",
            "Optimizer lr = 0.0195312\n",
            "\n",
            "я знаю вы меня не знаешь \n",
            "я в детстве в детстве к вам пришол \n",
            "я буду всех вас в первый в космос \n",
            "с вас не с тобой не буду я \n",
            "\n",
            "[43 / 300] Train: Loss = 3.55361, PPX = 34.94\n",
            "[43 / 300]   Val: Loss = 4.26014, PPX = 70.82\n",
            "Optimizer lr = 0.00488281\n",
            "\n",
            "олег придумал слово правду \n",
            "сегодня ночью умерла \n",
            "олег то голос то не знает \n",
            "как будто бы не ты не ты \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-155-7bb37f51ed84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_with_patterns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_with_patterns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_patterns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munk_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munk_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpad_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-153-15e41b8fd77b>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(model, criterion, optimizer, train_iter, use_patterns, epochs_count, unk_idx, pad_idx, val_iter)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mname_prefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'[{} / {}] '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdo_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munk_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_patterns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname_prefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'Train:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mval_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-153-15e41b8fd77b>\u001b[0m in \u001b[0;36mdo_epoch\u001b[0;34m(model, criterion, data_iter, unk_idx, pad_idx, use_patterns, optimizer, name)\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m                     \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "7lTAOQ8S2es9",
        "colab_type": "code",
        "outputId": "841da94b-0c52-4c09-e9b9-6387d0108813",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2651
        }
      },
      "cell_type": "code",
      "source": [
        "model_with_patterns_and_common_loss = LMModel(vocab_size=len(train_iter.dataset.fields['text'].vocab), pattern_vocab_size=len(train_iter.dataset.fields['pattern'].vocab)).to(DEVICE) # \n",
        "\n",
        "pad_idx = train_iter.dataset.fields['text'].vocab.stoi['<pad>']\n",
        "unk_idx = train_iter.dataset.fields['text'].vocab.stoi['<unk>']\n",
        "criterion = nn.CrossEntropyLoss(reduction='none').to(DEVICE)\n",
        "\n",
        "optimizer = optim.SGD(model_with_patterns_and_common_loss.parameters(), lr=20., weight_decay=1e-6)\n",
        "\n",
        "fit(model_with_patterns_and_common_loss, criterion, optimizer, train_iter, use_patterns=True, epochs_count=300, unk_idx=unk_idx, pad_idx=pad_idx, val_iter=test_iter)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 / 300] Train: Loss = 3.34254, PPX = 28.29\n",
            "[1 / 300]   Val: Loss = 3.02771, PPX = 20.65\n",
            "\n",
            "олег (-+) в (-+) не (-+) к (-) идёш (+) в (-) женщин (+) а (\\n) и (-) в (+) то (-+) не (-) знаю (+) и (-) то (+) что (-) просто () это (+) \n",
            "и (-+) на (-+) поедем (-+-) в (+) руке (\\n) \n",
            "[2 / 300] Train: Loss = 2.94664, PPX = 19.04\n",
            "[2 / 300]   Val: Loss = 2.89873, PPX = 18.15\n",
            "\n",
            "как (-+-) мы (+-) как (+-) секса (+) такое () олегу (-+) же (-) умер () нам () рядом () нам (+) же () же (-+) же (\\n) он () другим (-+) за (-+) прохожих (-+) я (-) с () кем (+) же () чувства (\\n) \n",
            "[3 / 300] Train: Loss = 2.83143, PPX = 16.97\n",
            "[3 / 300]   Val: Loss = 2.79544, PPX = 16.37\n",
            "\n",
            "у (-+) меня (-+) уже (-+) не (-) любит (+-) секса () жить () людям () нам () так () нам () терапевт () вам () членом () вам () так () ты () вам () вам () вам () вам () так () мы () кем (-) пор (+) как (-) ждал (+) \n",
            "а () то (+-) я (+-) буду () так (+) же (\\n) он (-) думал (+-+) что (-) я (+) мне (-) больше () вами (+) \n",
            "но (-) на (+) столе (-+-) и (+) не (-) надо (+-) запах () тоже (+-) рядом () людям () же (+-) надо () людям (+-) просто () б () так () мы () я (+) в () ней (-+) же (\\n) \n",
            "[4 / 300] Train: Loss = 2.74688, PPX = 15.59\n",
            "[4 / 300]   Val: Loss = 2.74093, PPX = 15.50\n",
            "\n",
            "я (-) не (+) могу (-+-) человека (+) олега (-) денег (+) мне (-) знаю () людям (+) так (-) умер (+) то (-) в () ухо (+) то (-) в () доме () друга (+) то (-) в () мире (+) то (-) в () то (+) то (-) в () мире (+) то (-) делать () мы (+) как (-) трудно () нам (+) же (\\n) я (-) буду (+) бы () ты (-) в (+) работу (-) что (+) то (-) надо () космос (+) то (-) в () мире (+) то (-) надо (+) то (-) в (+) квартире (-) думать (+) то (-) в () школе (+) то (-) в () мире (+) то (-) в () доме (+) же (-) вечер (+) то (-) в () море (+) то (-) в () мире (+) то (-) в () доме (+) то (-) в (+) хуй (\\n) и (-) я (+) не (-+) с () тех (-) пор (+) мы (-) понял () вам (+) не (-) видел (+) вы (-) ладно () вам (+) \n",
            "ты (</s>) в () чём (+-) время (+) \n",
            "\n",
            "[5 / 300] Train: Loss = 2.67802, PPX = 14.56\n",
            "[5 / 300]   Val: Loss = 2.70717, PPX = 14.99\n",
            "\n",
            "олег (-) в (+) работу (-+-+) ребята (-) дома (+) же (-) мясо () банок (+) же (-) сиськи () гости (+) он (-) умер () людям (+) же (-) ясно () людям (+) же (-) он () людям (+) он (-) умер () людям (+) он (-) видит () е (+) он (-) с () ним (+) \n",
            "ни (-) ней (+-) в () доме (+) же (-) руки (+) \n",
            "и () руки (+-) были (+) в (-) лоджии (+) и (-) в () море (+-) ходит () людям (+) кто (-) умер () людям (+) он () он (-) умер () не (+) \n",
            "\n",
            "[6 / 300] Train: Loss = 2.61863, PPX = 13.72\n",
            "[6 / 300]   Val: Loss = 2.66234, PPX = 14.33\n",
            "\n",
            "в (-) небе (+-) в (+-) телевизор (+-) люди () море (+) и (-) в (+) ночь (-) кофе () и (+) в () постели (-) минус (+) я (-) в () небо (+-) утром (+) оделся (-) в () руки (+-) я (+) в () окно (-) вижу (+) \n",
            "я (-) не (+) могу (-+) тебя (-) как (+) нормально (-) к (+) мне (-) яйца (+) но (-) тут () ты (+-) в () жызни (+) то (-) ль () так (+) \n",
            "\n",
            "[7 / 300] Train: Loss = 2.56493, PPX = 13.00\n",
            "[7 / 300]   Val: Loss = 2.64737, PPX = 14.12\n",
            "\n",
            "на (-+) сергея (-+-) павел (+-) птицы () ольгой (+) я (-) в () сердце (+) же (-) в () школу (+) же (-) этот () кофе (+) в () свой (-+) квартире (-+) \n",
            "а (-) ты (+-) так (+-) не (+-) в () смысле (+) что (-) в () ухо (+) ты (-) умер () вам (+) не (-) в () шутку (+) я () не (-) в () смысле (+) \n",
            "\n",
            "[8 / 300] Train: Loss = 2.51462, PPX = 12.36\n",
            "[8 / 300]   Val: Loss = 2.63263, PPX = 13.91\n",
            "\n",
            "мы (-) будем (+-+) все (-+) сказали (-) счастье () мысли (+) по (-) морде () гости (+) и () \n",
            "меня (-) в (+-) свет (+-+) и (-) в () каждой (+) же (-) космос (+) же (-) снова () небо (+) то (-) в () море (+) то (-) в () небо (+) то (-) вниз () ль (+) то () ль () то (-) в () то (+) \n",
            "\n",
            "[9 / 300] Train: Loss = 2.46516, PPX = 11.77\n",
            "[9 / 300]   Val: Loss = 2.63511, PPX = 13.94\n",
            "Optimizer lr = 5\n",
            "\n",
            "я (-+) не (-+) с (-+) работы (-) секса () первым () людям () водкой () е () людям () он () так () так () так () так () так () так () так () так () так () так () так () так () так () так () так () так () так () так () так () так () так () так () так () так () так () так () так () так () ножом () днем (-+) пор (-) так () так () так () так () так () так () так () так (-) так () так () так () так () так () так () так () так () так (-) так () так () так (+-+) \n",
            "так (-+) не (-+) с (-) вами (+-) чаю () коме (+) а (-) ты () я (+) ль (-) в (+) \n",
            "\n",
            "[10 / 300] Train: Loss = 2.33271, PPX = 10.31\n",
            "[10 / 300]   Val: Loss = 2.57377, PPX = 13.12\n",
            "\n",
            "у (-+) меня (-+) внутри (-+-) праздник (+) мне (-) ночью (+) я () заходит (-) с () водкой (+) с () утра (-) в () баню (+) по () ней (-) по () ней (+) \n",
            "и (-) не (+) могу (-+-) только (+) с () тобою (-) вместе (+) быть (-+) я () так () же (-) в (+) работу (-+) я () вам (-) в (+) \n",
            "\n",
            "[11 / 300] Train: Loss = 2.30804, PPX = 10.05\n",
            "[11 / 300]   Val: Loss = 2.57197, PPX = 13.09\n",
            "\n",
            "я (-) с (+-+-) смерти (+-) бога () богом (+) их () папой (-) думал (+) бы (-) в () мире (+) ни (-) в () чем (+) вы (\\n) в () нём (-) где (+) есть (-) в (+) хуй (-+) \n",
            "а (-) я (+) в (-) этом (+-) месте (+) мужчина (-) знаю (+) я (-) понял (+) ты (-) что (+) ты () вы (-) в (+) хуй () я (-) с (+) тобою (-) вам (+) мы (-) были () другом (+) \n",
            "\n",
            "[12 / 300] Train: Loss = 2.28911, PPX = 9.87\n",
            "[12 / 300]   Val: Loss = 2.57110, PPX = 13.08\n",
            "\n",
            "я (-+) не (-+) с (-+-) секса (+) по () субботу (-) с () кем (+) то (-) в () мире (+) то (-) в () доме (+) \n",
            "я (-) не (+-) знаю (+-) что (+) ты (-) скажешь () эти (+) же (-) дело (+) здесь () \n",
            "\n",
            "[13 / 300] Train: Loss = 2.27102, PPX = 9.69\n",
            "[13 / 300]   Val: Loss = 2.56937, PPX = 13.06\n",
            "\n",
            "когда (-) ты (+) мне (-+) не (-+-) скучно () больше () папой (+) с () олегом (-) в () тот (+) же (-) миг (+) \n",
            "сегодня (-+) я () не (-) как (+-) это () вам (+) я (-) в () детстве (+) то () же (-) аську (+) я () не (-) знал () я (+) в () \n",
            "\n",
            "[14 / 300] Train: Loss = 2.25302, PPX = 9.52\n",
            "[14 / 300]   Val: Loss = 2.57400, PPX = 13.12\n",
            "Optimizer lr = 1.25\n",
            "\n",
            "в (-) этом (+) как (-+) я (-+-) знаю () люди (+) ты () в () вагоне (-+) по (-) пояс (+) по () стране (-+) по (\\n) снегу (-) вижу (+-) час () не (+) был () не (-) надо () о (+) мужчиной (-) я (+) \n",
            "то (-) как (+) то (-) умер (+) но (-) кто (+) то (-) был (+) бы () \n",
            "и (-) вот (+) когда (-+) то (-+) под () ней (-+) я () живой (-) то (+) что (-) умер (+) с () ним (-+) \n",
            "\n",
            "[15 / 300] Train: Loss = 2.21104, PPX = 9.13\n",
            "[15 / 300]   Val: Loss = 2.56755, PPX = 13.03\n",
            "\n",
            "я (-) помню (+) свою (-) смерть (+) из (-) вашей (+) читаю (-) руку (+) мне () ноги () ноги () ноги () ноги () ноги () ноги () ноги () ноги () ноги () ноги () е () ноги (-) в () руки (+-) с () руки (+) и (-) с (+) \n",
            "и (-) в (+-) ночь (+-) и (+) в () постели (-) утром (+) а (-) в () ночь (+-) ночью () я () папой () я () не () так () так (-) я () не () так () так () так () так () так () так () так () так () так () так () так (-) в (+) ночь (\\n) я () не (-) надо (+) не (-) умер () я (+) не (-) в () сексе (+) \n",
            "\n",
            "[16 / 300] Train: Loss = 2.20319, PPX = 9.05\n",
            "[16 / 300]   Val: Loss = 2.56870, PPX = 13.05\n",
            "Optimizer lr = 0.3125\n",
            "\n",
            "я (-) всё (+) сказал (-+) и (-) вдруг (+) внезапно (-) скажет (+) я (-) понял (+) я (-) понял (+) я (-) знаю () думал (+) я (-) слышу (+) ты () ты (-) в (+) хуй (\\n) я () так (-) с (+) работы (-+) я (\\n) я () был (-) в () день (+) рожденья (-) в (+) мой (\\n) как () я (-) в () жопу (+-) был (+) я () не (-) в (+) \n",
            "и (-+) в (-) этом (+) в (-) этом (+) я () не (\\n) \n",
            "[17 / 300] Train: Loss = 2.19023, PPX = 8.94\n",
            "[17 / 300]   Val: Loss = 2.56915, PPX = 13.05\n",
            "Optimizer lr = 0.078125\n",
            "\n",
            "я (-) думал (+) кто (-) любит (+-) сон (+) не (-) страшен () а (+) вы (-+) я () я (-) в () трубку (+) же (-) не (+) был (\\n) я () не (-) буду () я (+) не () \n",
            "я (-) в () белом (+-) и (+-) в () нашем (+) субботу (-) с () жопой (+) вы (-) в () смысле (+) вы (-) в () смысле () с (+) олегом (-+) \n",
            "\n",
            "[18 / 300] Train: Loss = 2.18633, PPX = 8.90\n",
            "[18 / 300]   Val: Loss = 2.56994, PPX = 13.07\n",
            "Optimizer lr = 0.0195312\n",
            "\n",
            "мне (-) дали (+-) руки (+-+) и (-) сердце (+) же (-) крылья (+) и (-) звезды () воду (+) и () там () там (-) я (+) в () нем (-) где (+) я (-) вижу (+) он (-) понял (+) что (-) в (+) пол (-+) \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-172-78c4bd38be74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_with_patterns_and_common_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_with_patterns_and_common_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_patterns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munk_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munk_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpad_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-157-636c64af97f5>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(model, criterion, optimizer, train_iter, use_patterns, epochs_count, unk_idx, pad_idx, val_iter)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mname_prefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'[{} / {}] '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdo_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munk_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_patterns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname_prefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'Train:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mval_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-157-636c64af97f5>\u001b[0m in \u001b[0;36mdo_epoch\u001b[0;34m(model, criterion, data_iter, unk_idx, pad_idx, use_patterns, optimizer, name)\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m                     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m                     \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "r_YtM4ms8v--",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Задание** Добавьте маскинг `<unk>` токенов при тренировке модели."
      ]
    },
    {
      "metadata": {
        "id": "LzGwmgVf9Dkg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Улучшаем модель"
      ]
    },
    {
      "metadata": {
        "id": "BHneb8br9WXh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Tying input and output embeddings\n",
        "\n",
        "В модели есть два эмбеддинга - входной и выходной. Красивая и полезная в жизни идея - учить только одну матрицу, расшаренную между ними: [Using the Output Embedding to Improve Language Models](http://www.aclweb.org/anthology/E17-2025).\n",
        "\n",
        "От идеи одни плюсы: получается намного меньше обучаемых параметров и при этом достаточно заметно более высокое качество.\n",
        "\n",
        "**Задание** Реализуйте это. Достаточно написать что-то типа этого в конструкторе:\n",
        "\n",
        "`self._out_layer.weight = self._emb.weight`"
      ]
    },
    {
      "metadata": {
        "id": "N8I3QC4a_a8q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Добавление информации в выборку\n",
        "\n",
        "Сейчас у нас каждое слово предствляется одним индексом. Модели очень сложно узнать, сколько в нем слогов - а значит, сложно генерировать корректное стихотворение.\n",
        "\n",
        "На самом деле к каждому слову можно приписать кусочек из метрического шаблона:\n",
        "\n",
        "![](https://hsto.org/web/59a/b39/bd0/59ab39bd020c49a78a12cbab62c80181.png =x200)\n",
        "\n",
        "**Задание** Обновите функцию `read_poem`, пусть она генерирует два списка - список слов и список кусков шаблона.  \n",
        "Добавьте в модель вход - последовательности шаблонов, конкатенируйте их эмбеддинги со словами.  \n",
        "Дополнительная идея - заставьте модель угадывать, какой шаблон должен идти следующим (где-то половина будет подходящими, остальные - нет). Добавьте дополнительные потери от угадывания шаблона."
      ]
    },
    {
      "metadata": {
        "id": "MBX4NjzZ-0Hc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Увеличиваем выборку\n",
        "\n",
        "У нас есть выборка для пирожков, которая заметно больше.\n",
        "\n",
        "**Задание** Обучитесь на ней.\n",
        "\n",
        "### Transfer learning\n",
        "\n",
        "Простой и приятный способ улучшения модели - сделать перенос обученной на большом корпусе модели на меньшего объема датасет.\n",
        "\n",
        "Популярен этот способ больше в компьютерном зрении: [Transfer learning, cs231n](http://cs231n.github.io/transfer-learning/) - там есть огромный ImageNet, на котором предобучают модель, чтобы потом заморозить нижние слои и заменить выходные. В итоге модель использует универсальные представления данных, выученные на большом корпусе, но для предсказания совсем других меток - и качество очень здорово растет.\n",
        "\n",
        "Нам такие извращения пока не нужны (хотя потом пригодятся, ключевые слова: ULMFiT, ELMo и компания). Просто возьмем обученную на большем корпусе модель и поучим ее на меньшем корпусе. Ей всего-то нужно новый матрический шаблон последней строки выучить.\n",
        "\n",
        "**Задание** Обученную в прошлом пункте модель дообучите на порошки.\n",
        "\n",
        "### Conditional language model\n",
        "\n",
        "Ещё лучше - просто учиться на обоих корпусах сразу. Объедините пирожки и порошки, для каждого храните индекс 0/1 - был ли это пирожок или порошок. Добавьте вход - этот индекс и конкатенируйте его либо к каждому эмбеддингу слов, либо к каждому выходу из LSTM.\n",
        "\n",
        "**Задание** Научите единую модель, у которой можно просить сгенерировать пирожок или порошок."
      ]
    },
    {
      "metadata": {
        "id": "WnP743CM-bY6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Variational & word dropout\n",
        "\n",
        "**Задание** На прошлом занятии приводились примеры более приспособленных к RNN'ам dropout'ов. Добавьте их.\n",
        "\n",
        "**Задание** Кроме этого, попробуйте увеличивать размер модели или количество слоев в ней, чтобы улучшить качество."
      ]
    },
    {
      "metadata": {
        "id": "Ejqx6BC0JcG2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Multi-task learning\n",
        "\n",
        "Ещё один важный способ улучшения модели - multi-task learning. Это когда одна модель учится делать предсказания сразу для нескольких задач.\n",
        "\n",
        "В нашем случае это может быть предсказанием отдельно леммы слова и отдельно - его грамматического значения:\n",
        "![](https://hsto.org/web/e97/8a8/6e8/e978a86e8a874d8d946bb15e6a49a713.png =x350)\n",
        "\n",
        "В итоге модель выучивает как языковую модель по леммам, так и модель POS tagging'а. Одновременно!\n",
        "\n",
        "Возьмем корпус из universal dependencies - он уже размечен, как нужно.\n",
        "\n",
        "Почитаем его:"
      ]
    },
    {
      "metadata": {
        "id": "YT-kzC2_KuLX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from corpus_iterator import Token, CorpusIterator\n",
        "\n",
        "fields = [('word', Field()), ('lemma', Field()), ('gram_val', Field())]\n",
        "examples = []\n",
        "\n",
        "with CorpusIterator('UD_Russian-SynTagRus/ru_syntagrus-ud-train.conllu') as corpus_iter:\n",
        "    for sent in corpus_iter:\n",
        "        words = ['<s>'] + [tok.token.lower() for tok in sent] + ['</s>']\n",
        "        lemmas = ['<s>'] + [tok.lemma.lower() for tok in sent] + ['</s>']\n",
        "        gr_vals = ['<s>'] + [tok.grammar_value for tok in sent] + ['</s>']\n",
        "        examples.append(Example.fromlist([words, lemmas, gr_vals], fields))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l_3xaD-2KwNW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print('Words:', examples[1].word)\n",
        "print('Lemmas:', examples[1].lemma)\n",
        "print('Grammar vals:', examples[1].gram_val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HcGm5fPsLESH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Таким образом, размер словаря может быть существенно сокращен - лемм меньше, чем слов, а предсказание грамматики вынуждает модель быть более осведомленной о согласовании слов."
      ]
    },
    {
      "metadata": {
        "id": "xZe5HimdLb9i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dataset = Dataset(examples, fields)\n",
        "\n",
        "dataset.fields['word'].build_vocab(dataset, min_freq=3)\n",
        "print('Word vocab size =', len(dataset.fields['word'].vocab))\n",
        "dataset.fields['lemma'].build_vocab(dataset, min_freq=3)\n",
        "print('Lemma vocab size =', len(dataset.fields['lemma'].vocab))\n",
        "dataset.fields['gram_val'].build_vocab(dataset)\n",
        "print('Grammar val vocab size =', len(dataset.fields['gram_val'].vocab))\n",
        "\n",
        "train_dataset, test_dataset = dataset.split(split_ratio=0.75)\n",
        "\n",
        "train_iter, test_iter = BucketIterator.splits(datasets=(train_dataset, test_dataset), batch_sizes=(32, 128), \n",
        "                                              shuffle=True, device=DEVICE, sort=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y7xlr15lLm78",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Построим маппинг из пары (лемма, грамматическое значение) в слово - если бы у нас под рукой был морфологический словарь, маппинг можно было бы пополнить, добавить слова для лемм из корпуса, которые не встретились в обучении."
      ]
    },
    {
      "metadata": {
        "id": "_AvT2MgeLmP8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dictionary = {\n",
        "    (lemma, gr_val): word\n",
        "    for example in train_iter.dataset.examples \n",
        "    for word, lemma, gr_val in zip(example.word, example.lemma, example.gram_val)\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VaP8Krx1LeJl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Задание**  Обновите генератор - например, можно сэмплировать лемму и находить самое вероятное грамматическое значение, которое встречается  в паре с этой леммой в `dictionary`."
      ]
    },
    {
      "metadata": {
        "id": "PeBH0WYjMQ5h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generate(model, temp=0.7):\n",
        "    ..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w3GzOZ8dMVMJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Задание** Обновите модель и функцию обучения.\n",
        "\n",
        "Модель должна принимать пары `lemma, gr_val`, конкатенировать их эмбеддинги и предсказывать следующие `lemma, gr_val` по выходу из LSTM.\n",
        "\n",
        "Функция `do_epoch` должна суммировать потери по предсказанию леммы (делая маскинг для `<unk>` и `<pad>`) + потери по предсказанию грамматического значения (с маскингом по `<pad>`)."
      ]
    },
    {
      "metadata": {
        "id": "vL2xPe-BNRhu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Контролируемая генерация\n",
        "\n",
        "Хочется сделать генерацию более контролируемой - в идеале, задавать тему.\n",
        "\n",
        "Простой способ - сделать тематическое моделирование и найти в текстах какие-то темы - а потом передавать вектор тем вместе с эмбеддингом слова, чтобы модель училась генерировать тематически-согласованный текст."
      ]
    },
    {
      "metadata": {
        "id": "exopH1jlN4fc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from gensim import corpora, models\n",
        "\n",
        "docs = [[word for word in poem if word != '\\\\n'] for poem in perashki]\n",
        "\n",
        "dictionary = corpora.Dictionary(docs)\n",
        "dictionary.filter_n_most_frequent(100)\n",
        "\n",
        "bow_corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
        "\n",
        "lda_model = models.LdaModel(bow_corpus, num_topics=5, id2word=dictionary, passes=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LLPO9U1-Pakp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Посмотреть, что выучилось, можно так:"
      ]
    },
    {
      "metadata": {
        "id": "hzEg-8SZs8t7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pyLDAvis\n",
        "import pyLDAvis.gensim\n",
        "\n",
        "pyLDAvis.enable_notebook()\n",
        "pyLDAvis.gensim.prepare(lda_model, bow_corpus, dictionary)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mHx1GJrWPkM8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Предсказывает распределение модель как-то так:"
      ]
    },
    {
      "metadata": {
        "id": "rTD0CGMdPsF5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for word in perashki[10]:\n",
        "    if word == '\\\\n':\n",
        "        print()\n",
        "    else:\n",
        "        print(word, end=' ')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0m0b6i2MPlKD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "lda_model.get_document_topics(bow_corpus[10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-imKaGGUQM5K",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Задание** Посчитайте для всех текстов вектора тем, передавайте их вместе со словами (конкатенируя к эмбеддингам). Посмотрите, вдруг чего получится."
      ]
    },
    {
      "metadata": {
        "id": "w8V0KAz_CNf0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Дополнительные материалы\n",
        "\n",
        "## Статьи\n",
        "\n",
        "Regularizing and Optimizing LSTM Language Models, 2017 [[arxiv]](https://arxiv.org/abs/1708.02182), [[github]](https://github.com/salesforce/awd-lstm-lm) - одна из самых полезных статей про языковые модели + репозиторий, в котором реализовано много полезного, стоит заглянуть\n",
        "\n",
        "Exploring the Limits of Language Modeling, 2016 [[arxiv]](https://arxiv.org/abs/1602.02410)\n",
        "\n",
        "Using the Output Embedding to Improve Language Models, 2017 [[pdf]](http://www.aclweb.org/anthology/E17-2025)\n",
        "\n",
        "\n",
        "## Transfer learning\n",
        "[Transfer learning, cs231n](http://cs231n.github.io/transfer-learning/)  \n",
        "[Transfer learning, Ruder](http://ruder.io/transfer-learning/) - очень подробная статья от чувака из NLP\n",
        "\n",
        "## Multi-task learning\n",
        "[An Overview of Multi-Task Learning in Deep Neural Networks, Ruder](http://ruder.io/multi-task/)  \n",
        "[Multi-Task Learning Objectives for Natural Language Processing, Ruder](http://ruder.io/multi-task-learning-nlp/)"
      ]
    },
    {
      "metadata": {
        "id": "Vwb5e5hPQebd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Сдача\n",
        "\n",
        "[Форма для сдачи](https://goo.gl/forms/ASLLjYncKUcIHmuO2)  \n",
        "[Feedback](https://goo.gl/forms/9aizSzOUrx7EvGlG3)"
      ]
    }
  ]
}