{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week 07 - Language Models.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "OE7fXh-OSJYF",
        "colab_type": "code",
        "outputId": "7e104b1f-adaf-40ab-87eb-ced2b2bded1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "cell_type": "code",
      "source": [
        "!pip3 -qq install torch==0.4.1\n",
        "!pip -qq install torchtext==0.3.1\n",
        "!wget -qq --no-check-certificate 'https://drive.google.com/uc?export=download&id=1Pq4aklVdj-sOnQw68e1ZZ_ImMiC8IR1V' -O tweets.csv.zip\n",
        "!wget -qq --no-check-certificate \"https://drive.google.com/uc?export=download&id=1ji7dhr9FojPeV51dDlKRERIqr3vdZfhu\" -O surnames.txt\n",
        "!unzip tweets.csv.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tcmalloc: large alloc 1073750016 bytes == 0x5919a000 @  0x7f3bcb05b2a4 0x594e17 0x626104 0x51190a 0x4f5277 0x510c78 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f6070 0x510c78 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f6070 0x4f3338 0x510fb0 0x5119bd 0x4f6070\n",
            "Archive:  tweets.csv.zip\n",
            "  inflating: tweets.csv              \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uhvfH55PUJ8K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    from torch.cuda import FloatTensor, LongTensor\n",
        "    DEVICE = torch.device('cuda')\n",
        "else:\n",
        "    from torch import FloatTensor, LongTensor\n",
        "    DEVICE = torch.device('cpu')\n",
        "\n",
        "np.random.seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jVcnkGDgxfNx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Языковые модели"
      ]
    },
    {
      "metadata": {
        "id": "8Kjg1Z3xxmEP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "*Языковая модель* - это штука, которая умеет оценивать вероятности встретить последовательность слов $w_1, \\ldots, w_n$:   \n",
        "$$\\mathbf{P}(w_1, \\ldots, w_n) = \\prod_k \\mathbf{P}(w_k|w_{k-1}, \\ldots, w_{1}).$$\n",
        "\n",
        "Интерпретируемы и интересны тут именно условные вероятности - какое слово языковая модель ожидает вслед за данными. У нас у всех такая языковая модель есть, так-то. Например, в таком контексте\n",
        "\n",
        "![](https://hsto.org/web/956/239/601/95623960157b4e15a1b3f599aed62ed2.png =x170)\n",
        "\n",
        "моя языковая модель говорит - после *честных* навряд ли пойдёт *мой*. А вот *и* или, конечно, *правил* - очень даже.\n",
        "\n",
        "А задача такая: научиться генерировать политические твиты по образу и подобию `Russian Troll Tweets`. Датасет взят отсюда: https://www.kaggle.com/vikasg/russian-troll-tweets"
      ]
    },
    {
      "metadata": {
        "id": "JpjfUoN4_WY7",
        "colab_type": "code",
        "outputId": "67444f44-69db-47ff-bf46-a38e818b475d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv('tweets.csv')\n",
        "\n",
        "data.text.sample(15).tolist()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['RT @RivalThoughts: @CNN This lack of forethought is what ran the country into the ground.',\n",
              " \"#TopNews Icahn on CNBC:'Archie Bunker of the world' will vote for Trump\",\n",
              " 'RT @mclark1951: Just 5.7 Percent Of #Clinton Foundation Budget Actually Went To Charitable Grants https://t.co/c2EIYW4w9H #uniteblue2016 #p…',\n",
              " \"RT @JamilSmith: Read @jelani9 on Rudy Giuliani's flailing effort to gain relevance in this Trump moment. https://t.co/48p9d31pgi\",\n",
              " 'RT @Laura_A_Diaz: #StandUpWithEvan #MakeHistoryWithEvan  Unite and #Vote3rdParty #Deny270 https://t.co/quB1FdZZAo',\n",
              " 'RT @MommyExchangeGa: Camouflage Wedding Rings Made From Titanium! High Quality. Choose from Promise, Wedding, Friendship and Couples https:…',\n",
              " 'RT @NewssTrump: BREAKING: Trump’s UN Ambassador Just Put The Fear Of God In Our Enemies! She Just Gave The UN Teeth For The First… https://…',\n",
              " '@Nero March for Trump at Trump tower NY happening now:\\n#Trump #MAGA https://t.co/SWvbWxOEjO',\n",
              " \"Why don't Portuguese Muslims speak out and condemn this guy? If he doesn't represent Islam hasn't he offended them?… https://t.co/3hWEBCNgyg\",\n",
              " 'RT @theclobra: People either live in anonymity and then attack people for putting themselves out there or they are hypocrites and do the sa…',\n",
              " 'RT @GoldStarMomTX55: ChristiChat: RT ChristiChat: Dem IRONY!\\r\\nbillclinton address in 1995 “We are a nation of immigrants, but we are als… ht…',\n",
              " 'RT @pollygolightly: Boundaries. #GiftIdeasForPoliticians https://t.co/N0EN6hvVaD',\n",
              " 'RT @TheYoungTurks: .@HillaryClinton was caught on tape discussing rigging an election. https://t.co/a3WTZ0fIXz',\n",
              " 'RT @FeministaJones: Old Bay Macaroni and Cheese https://t.co/8yyIOxMmUh',\n",
              " 'RT @mansplainer123: Iran said it only takes 7min to hit Tel-Aviv.We need to point out that it will take 45min after that to turn them and t…']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "WAQ4d__2_sAz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Да, результаты будут упороты, сразу предупреждаю."
      ]
    },
    {
      "metadata": {
        "id": "7Qvqidof7Fsi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Чтение данных"
      ]
    },
    {
      "metadata": {
        "id": "OSu56oDX-KY5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Кого-нибудь уже достало писать все эти построения батчей, словари - вот это всё? Лично меня - да!\n",
        "\n",
        "В pytorch есть специальный класс для генерации батчей - `Dataset`. Вместо того, чтобы писать функцию типа `iterate_batches`, можно отнаследовать от него и переопределить методы `__len__` и `__getitem__`... и реализовать в них почти всё то, что было в `iterate_batches`. Пока не впечатляет, да?\n",
        "\n",
        "Ещё там есть `DataLoader`, умеющий работать с датасетом. Он позволяет делать shuffle батчей и генерацию их в отдельных процессах - это особенно важно, когда генерация батча - долгая операция. Например, в картинках. Почитать про это всё можно здесь: [Data Loading and Processing Tutorial](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html).\n",
        "\n",
        "Но пока что всё равно не особо круто, мне кажется. Интересно другое - у pytorch в репозитории живет отдельная библиотечка - [torchtext](https://github.com/pytorch/text). Вот она уже даст нам специальные реализации `Dataset` для работы с текстом и всякие тулзы, делающие жизнь чуточку проще.\n",
        "\n",
        "Библиотеке, на мой взгляд, недостает туториалов, в которых бы показывалось, как с ней работать - но можно читать исходный код, он приятный.\n",
        "\n",
        "План такой: построить класс `torchtext.data.Dataset`, для него создать итератор, и учить модель.\n",
        "\n",
        "Данный датасет инициализируется двумя параметрами:\n",
        "```\n",
        "            examples: List of Examples.\n",
        "            fields (List(tuple(str, Field))): The Fields to use in this tuple. The\n",
        "                string is a field name, and the Field is the associated field.\n",
        "```\n",
        "Разберемся сначала со вторым.\n",
        "\n",
        "`Field` - это такая мета-информация для датасета + обработчик сэмплов.  \n",
        "\n",
        "Он имеет кучу параметров, на которые проще посмотреть [здесь](https://github.com/pytorch/text/blob/master/torchtext/data/field.py). Если коротко, то он может предобрабатывать (например, токенизировать) предложения, строить словарь (отображение из слова в индекс), строить батчи - добавлять паддинги и конвертировать в тензоры. Что ещё нужно в жизни?\n",
        "\n",
        "Мы будем делать character-level языковую модель, поэтому токенизация для нас - превращение строки в набор символов. Попросим также добавлять в начало и конец спец-символы `<s>` и `</s>`."
      ]
    },
    {
      "metadata": {
        "id": "ilAMVxA8Xy4L",
        "colab_type": "code",
        "outputId": "9bb6ceea-8d75-49fb-bb7c-0eaf8b3ff3b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "from torchtext.data import Field\n",
        "\n",
        "text_field = Field(init_token='<s>', eos_token='</s>', lower=True, tokenize=lambda line: list(line))\n",
        "text_field"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torchtext.data.field.Field at 0x7f9fe1b25c88>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "L_i0Z6JhF0rA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Препроцессинг будет выглядеть так:"
      ]
    },
    {
      "metadata": {
        "id": "B-8IPlPHFyKa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# text_field.preprocess(data.text.iloc[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "19NFhTSNF1_1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Сконвертируем всё и посмотрим на распределение длин:"
      ]
    },
    {
      "metadata": {
        "id": "wz1QnivMBmU3",
        "colab_type": "code",
        "outputId": "f8f3d015-74d4-4607-d218-f7400baf7c3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "data['text'] = data['text'].fillna('')\n",
        "lines = data.apply(lambda row: text_field.preprocess(row['text']), axis=1).tolist()\n",
        "\n",
        "lengths = [len(line) for line in lines]\n",
        "\n",
        "plt.hist(lengths, bins=30)[-1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<a list of 30 Patch objects>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD4CAYAAAAHHSreAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f9f85056b00>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "dE9rPW9UHE7d",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Отсечем слишком короткие строки и преобразуем оставшиеся в `Example`'ы:"
      ]
    },
    {
      "metadata": {
        "id": "jfTlpBxODBg8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from torchtext.data import Example\n",
        "\n",
        "lines = [line for line in lines if len(line) >= 50]\n",
        "\n",
        "fields = [('text', text_field)]\n",
        "examples = [Example.fromlist([line], fields) for line in lines]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7z1wPlz_HeEP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "По `Example` можно получить обратно все поля, которые мы туда запихнули. Например, сейчас мы создали одно поле `text`:"
      ]
    },
    {
      "metadata": {
        "id": "iGMRSuk_HYCm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# examples[0].text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Yef1bv2MQcEA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Построим, наконец, датасет:"
      ]
    },
    {
      "metadata": {
        "id": "gSccEmVIHAaQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from torchtext.data import Dataset\n",
        "\n",
        "dataset = Dataset(examples, fields)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vEe5YXIpRCYD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Датасет можно разбить на части:"
      ]
    },
    {
      "metadata": {
        "id": "21whmJDFRBV1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_dataset, test_dataset = dataset.split(split_ratio=0.75)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "14CyhugSQsOf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "По нему можно построить словарь:"
      ]
    },
    {
      "metadata": {
        "id": "NQs3jbhyQkJD",
        "colab_type": "code",
        "outputId": "60197d3a-bd24-468d-9587-46dcc295be66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "text_field.build_vocab(train_dataset, min_freq=30)\n",
        "\n",
        "print('Vocab size =', len(text_field.vocab))\n",
        "print(text_field.vocab.itos)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab size = 322\n",
            "['<unk>', '<pad>', '<s>', '</s>', ' ', 't', 'e', 'a', 'o', 'r', 'i', 's', 'n', 'l', 'h', 'c', 'p', 'd', 'm', 'u', '/', 'g', 'y', ':', 'w', 'b', 'f', '.', '@', 'k', 'v', '#', 'j', 'z', 'x', '\\n', \"'\", '…', '1', ',', '0', '2', 'q', '\\r', '!', '4', '6', '7', '3', '\"', '5', '-', '9', '8', '_', '?', ';', '’', '&', ')', '(', '‘', '“', '$', '😂', '|', '”', '*', '%', '–', '️', '🇸', '🇺', 'о', 'а', 'ü', '\\xa0', 'и', 'е', '▶', 'т', '~', 'н', '🔥', 'р', '+', 'ä', '💥', '[', ']', '—', 'л', 'с', '=', 'в', '🚨', 'é', 'к', 'п', 'м', '❤', 'д', 'ö', '🤔', '👍', '👇', '👏', '‼', '★', 'ا', 'у', '�', '🏻', '😭', '`', '🏾', '👉', 'б', 'з', '🙏', '🏼', 'ы', '😡', 'ь', '😊', 'г', 'я', 'ل', '😍', 'ß', '💯', '»', '•', '🏽', '🏿', '😎', 'й', 'ч', '😳', '✔', '💀', 'ي', 'م', 'à', '🙄', '✊', 'х', '💩', '➡', '🙌', '✨', 'ा', '🎉', '«', 'ж', '👊', 'ر', 'و', '💪', 'è', '⚡', '😘', 'ن', '😉', '😏', '😩', 'ш', '🔴', '💨', 'ю', '👀', 'े', '💰', 'ت', 'د', '►', 'क', '\\u200b', 'ب', '✅', '🚂', '➠', '🌟', '😱', '👌', 'र', '😁', 'ه', '🎄', '😅', '❗', '☺', 'の', '🎶', '👈', '😜', '😆', '´', '❌', 'ع', 'ц', '💙', 'י', '⏳', '😄', '🚫', 'س', 'š', '\\x92', 'स', '💕', '😒', 'ו', '😠', 'á', '\\u200d', 'い', '。', '🌹', '💞', 'ç', 'ê', 'म', '⭐', '🐾', 'ф', '🆘', '💜', 'ר', 'ح', 'ह', '„', '⬇', 'な', '😔', '🎈', '🎯', '💗', '🔫', 'ी', '✌', '🆓', '🗽', '😢', 'ो', '™', '♫', '👑', '💔', '💦', 'ı', 'ि', '्', '♥', '😈', '^', 'ž', 'ة', 'त', '☆', '👎', '🤣', 'ك', 'न', '😀', '\\\\', '⁉', '⚪', '❓', '、', '🏆', '💣', 'щ', 'ת', '❄', '😝', 'ć', 'ं', 'は', '🤘', '£', 'ल', '☀', 'と', '・', 'č', 'ف', '→', '😴', 'ñ', 'ج', '❣', '🇷', '🔹', '😷', 'í', 'ó', 'ق', 'प', '☕', '🌴', '🍀', '😑', '🤕', '🤗', '⤵', 'が', 'に', '😃', '😬', 'ग', 'ब', 'て', 'る', '🌌', '👆', '😇', '\\U000fe4e6', 'ה', '⃣', 'し', '🔶']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-_EAdgsWRTzj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Наконец, по нему можно итерироваться:"
      ]
    },
    {
      "metadata": {
        "id": "qaEMoxdVG98p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from torchtext.data import BucketIterator\n",
        "\n",
        "train_iter, test_iter = BucketIterator.splits(datasets=(train_dataset, test_dataset), batch_sizes=(32, 128), \n",
        "                                              shuffle=True, device=DEVICE, sort=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RMG4L1-5RXnb",
        "colab_type": "code",
        "outputId": "fca61d44-4737-4e6c-dabf-104e8731b74f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "cell_type": "code",
      "source": [
        "batch = next(iter(train_iter))\n",
        "\n",
        "batch"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "[torchtext.data.batch.Batch of size 32]\n",
              "\t[.text]:[torch.cuda.LongTensor of size 146x32 (GPU 0)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "oTrSUkqEhZzh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Перплексия"
      ]
    },
    {
      "metadata": {
        "id": "gqc9HpTM-FwD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Нашу задачу, как всегда, нужно начинать с двух вопросов - какую метрику оптимизируем и какой бейзлайн.\n",
        "\n",
        "С метрикой всё просто - мы хотим, чтобы модель как можно лучше умела приближать распределение слов языка. Всего языка у нас нету, поэтому обойдёмся тестовой выборкой.\n",
        "\n",
        "На ней можно посчитать кросс-энтропийные потери: \n",
        "$$H(w_1, \\ldots, w_n) = - \\frac 1n \\sum_k \\log\\mathbf{P}(w_k | w_{k-1}, \\ldots, w_1).$$\n",
        "\n",
        "Здесь вероятность $\\mathbf{P}$ - это вероятность, оцененная нашей языковой моделью. Идеальная модель давала бы вероятность равную 1 для слов в тексте и потери были бы нулевыми - хотя это, конечно, невозможно, даже вы же не можете предсказать следующее слово, что уж про бездушную машину говорить.\n",
        "\n",
        "Таким образом, всё как всегда - оптимизируем кросс-энтропию и стремимся сделать её как можно ниже.\n",
        "\n",
        "Ну, почти всё. Ещё есть отдельная метрика для языковых моделей - *перплексия*. Это просто возведенные в экспоненту кросс-энтропийные потери:\n",
        "\n",
        "$$PP(w_1, \\ldots, w_n) = e^{H(w_1, \\ldots, w_n)} = e^{- \\frac 1n \\sum_k \\log\\mathbf{P}(w_k | w_{k-1}, \\ldots, w_1)} = \\left(\\mathbf{P}(w_1, \\ldots, w_n) \\right)^{-\\frac 1n}.$$\n",
        "\n",
        "У её измерения есть некоторый сакральный смысл кроме банальной интепретируемости: представим модель, предсказывающую слова из словаря равновероятно вне зависимости от контекста. Для неё $\\mathbf{P}(w) = \\frac 1 N$, где $N$ — размер словаря, а перплексия будет равна размеру словаря — $N$. Конечно, это совершенно глупая модель, но оглядываясь на неё, можно трактовать перплексию реальных моделей как уровень неоднозначности генерации слова.\n",
        "\n",
        "Скажем, в модели с перплексией 100 выбор следующего слова также неоднозначен, как выбор из равномерного распределения среди 100 слов. И если такой перплексии удалось достичь на словаре в 100 000, получается, что удалось сократить эту неоднозначность на три порядка по сравнению с тупым рандомом."
      ]
    },
    {
      "metadata": {
        "id": "xW8I0lKv9y1H",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Бейзлайн"
      ]
    },
    {
      "metadata": {
        "id": "7wInBuBn-DIf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Вообще, бейзлайн тут тоже очень простой. Мы, на самом деле, даже смотрели его на курсе концепций: [N-граммная языковая модель](https://colab.research.google.com/drive/1lz9vO6Ue5zOiowEx0-koXNiejBrrnbj0). Можно подсчитывать вероятности N-грамм слов по частотностям их появления в обучающем корпусе. А дальше использовать аппроксимацию $\\mathbf{P}(w_k|w_1, \\ldots, w_{k-1}) \\approx \\mathbf{P}(w_k|w_{k-1}, \\ldots, w_{k-N + 1})$.\n",
        "\n",
        "Применим лучше сеточки для реализации того же.\n",
        "\n",
        "![](https://image.ibb.co/buMnLf/2018-10-22-00-22-56.png =x450)  \n",
        "*From cs224n, Lecture 8 [pdf](http://web.stanford.edu/class/cs224n/lectures/lecture8.pdf)*\n",
        "\n",
        "На вход приходит последовательность слов, они эмбеддятся, а дальше с помощью выходного слоя считается наиболее вероятное следующее слово.\n",
        "\n",
        "Стоп... Но мы же уже реализовывали такое! В Word2vec CBoW модели мы по контексту предсказывали центральное слово - единственное отличие в том, что теперь мы имеем только левый контекст. Значит, всё, идём к следующей модели?\n",
        "\n",
        "Нет! Тут ещё есть с чем развлечься. В Word2vec мы формировали батчи таким образом:\n",
        "![](https://image.ibb.co/bs3wgV/training-data.png =x350)  \n",
        "*From [Word2Vec Tutorial - The Skip-Gram Model, Chris McCormic](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)*\n",
        "\n",
        "То есть нарезали из текста набор пар <контекст, слово> (и как-то их использовали в зависимости от метода).\n",
        "\n",
        "Это нерационально - каждое слово повторяется много раз. Но можно использовать сверточные сети - они за нас применят операцию умножения на $W$ к каждому окну. В результате размер входного батча будет сильно меньше.\n",
        "\n",
        "Чтобы правильно всё обработать, нужно добавить паддинг в начало последовательности размером `window_size - 1` - тогда первое слово будет предсказываться по `<pad>...<pad><s>`.\n",
        "\n",
        "**Задание** Реализуйте языковую модель с фиксированным окном."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "A-tn_Gmi3pU0",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ConvLM(nn.Module):\n",
        "    def __init__(self, vocab_size, window_size=5, emb_dim=16, filters_count=128):\n",
        "        super().__init__()\n",
        "        \n",
        "        self._window_size = window_size\n",
        "        self._emb = nn.Embedding(vocab_size, emb_dim, padding_idx=1)\n",
        "        self._conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=1, out_channels=filters_count, kernel_size=(window_size, emb_dim)),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm2d(filters_count)\n",
        "        )\n",
        "        self._out = nn.Linear(filters_count, vocab_size)\n",
        "        \n",
        "    def forward(self, inputs): \n",
        "        padding = inputs.new_ones(self._window_size - 1, inputs.shape[1])\n",
        "        outputs = torch.cat((inputs, padding), dim=0)\n",
        "        outputs = self._emb(outputs)  # (seq_len, batch_size, emb_dim)\n",
        "        outputs = outputs.permute((1, 0, 2))  # (batch_size, seq_len, emb_dim)\n",
        "        outputs = outputs.unsqueeze(1) \n",
        "        outputs = self._conv(outputs)  # (batch_size, filters_count, input_size, 1)   \n",
        "        outputs = outputs.squeeze(-1) # (batch_size, filters_count, input_size)\n",
        "        outputs = outputs.permute(2, 0, 1)\n",
        "#         outputs = outputs.max(dim=2)[0]  # (batch_size, filters_count)\n",
        "        outputs = self._out(outputs)  # (input_size, batch_size, vocab_size)\n",
        "#         outputs = F.softmax(outputs, dim=2)\n",
        "        return outputs, None  # hacky way to use training cycle for RNN and Conv simultaneously"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bjpOLKBH5yS5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Проверим, что оно работает:"
      ]
    },
    {
      "metadata": {
        "id": "ks_RTZ14nMRz",
        "colab_type": "code",
        "outputId": "a9633e33-5d2b-4b5d-dbed-56b4878b03fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "model = ConvLM(vocab_size=len(train_iter.dataset.fields['text'].vocab)).to(DEVICE)\n",
        "\n",
        "model(batch.text)[0].shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([150, 32, 310])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "metadata": {
        "id": "Lb_2VTBW5v_7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Задание** Реализуйте функцию для сэмплирования последовательности из языковой модели."
      ]
    },
    {
      "metadata": {
        "id": "0oUg0BjV2JjE",
        "colab_type": "code",
        "outputId": "5f7cff3c-800b-4d7e-8c47-10fc6b6cbbd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "def sample(probs, temp):\n",
        "    probs = F.log_softmax(probs.squeeze(), dim=0)\n",
        "    probs = (probs / temp).exp()\n",
        "    probs /= probs.sum()\n",
        "    probs = probs.cpu().numpy()\n",
        "\n",
        "    return np.random.choice(np.arange(len(probs)), p=probs)\n",
        "\n",
        "\n",
        "def generate(model, temp=0.7):\n",
        "    model.eval()\n",
        "    \n",
        "    history = [train_dataset.fields['text'].vocab.stoi['<s>']]\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for _ in range(150):\n",
        "            inputs = LongTensor([history])\n",
        "            inputs = inputs.permute(1, 0)\n",
        "            logits, _ = model(inputs)\n",
        "            index = sample(logits[-1, 0], temp)\n",
        "            history.append(index)\n",
        "            print(train_dataset.fields['text'].vocab.itos[history[-1]], end='')\n",
        "            \n",
        "generate(model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "بy0👇w⬇。☆🎄☕зtb🔹とm、5`„😡🆓عב💯رá🌌🏾ה😢♥нф»г5]👎🚫ü💦😑ч🤘😂pnšי´ćع・👀о2💗ее£🔹م🔶?✔3💀💔👏💀ñ‘יنح😘⚡😍😜のщ🙄%г🎈ع😔😅😔✨#è💔.。ا@💰💗➠لtñ*תт✔lेo・🌹#د🆓💯(ि”👀❤в?´ल</s>мщع,😡ं🚫וرаk‍lмр_ю➖🎯%f💦म"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CXuN871a852l",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Задание** Мы до сих пор не задали ни какой target. А предсказывать нам будет нужно следующие слова - то есть просто сдвинутый на 1 входной тензор. Реализуйте построение target'а и подсчет потерь."
      ]
    },
    {
      "metadata": {
        "id": "7cmQFchzFMqn",
        "colab_type": "code",
        "outputId": "1a4eb6ec-8095-467d-80d5-15dcc8d872a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "logits = model(batch.text)[0]\n",
        "Y =  batch.text.view(-1)\n",
        "Y_hat = logits.view(-1, logits.shape[-1])\n",
        "\n",
        "Y_hat[range(Y_hat.shape[0]),  Y] # probs of corect values"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-0.1909, -0.0968, -0.0968,  ..., -0.0941, -0.0941, -0.0941],\n",
              "       device='cuda:0', grad_fn=<TakeBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "CGLkcXARjhTM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import math\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def do_epoch(model, criterion, data_iter, unk_idx, pad_idx, optimizer=None, name=None):\n",
        "    epoch_loss = 0\n",
        "    \n",
        "    is_train = not optimizer is None\n",
        "    name = name or ''\n",
        "    model.train(is_train)\n",
        "    \n",
        "    batches_count = len(data_iter)\n",
        "    \n",
        "    with torch.autograd.set_grad_enabled(is_train):\n",
        "        with tqdm(total=batches_count) as progress_bar:\n",
        "            for i, batch in enumerate(data_iter):\n",
        "                labels = batch.text[1:, :]\n",
        "                labels = labels.view(-1)\n",
        "                \n",
        "                logits, _ = model(batch.text)\n",
        "                logits = logits[:-1, :, :]\n",
        "                logits = logits.view(-1, logits.shape[-1])\n",
        "                \n",
        "                mask = ((labels != pad_idx) * (labels != unk_idx)).float()\n",
        "                \n",
        "                loss = torch.sum(criterion(logits, labels.view(-1)) * mask) / torch.sum(mask)\n",
        "                \n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "                if optimizer:\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    nn.utils.clip_grad_norm_(model.parameters(), 1.)\n",
        "                    optimizer.step()\n",
        "\n",
        "#                 if i > 100:\n",
        "#                     break\n",
        "#                     generate(model)\n",
        "#                     print()\n",
        "#                     print('{:>5s} Loss = {:.5f}, PPX = {:.2f}'.format(name, loss.item(), math.exp(loss.item())))\n",
        "                progress_bar.update()\n",
        "                progress_bar.set_description('{:>5s} Loss = {:.5f}, PPX = {:.2f}'.format(name, loss.item(), \n",
        "                                                                                         math.exp(loss.item())))\n",
        "                \n",
        "            progress_bar.set_description('{:>5s} Loss = {:.5f}, PPX = {:.2f}'.format(\n",
        "                name, epoch_loss / batches_count, math.exp(epoch_loss / batches_count))\n",
        "            )\n",
        "\n",
        "    return epoch_loss / batches_count\n",
        "\n",
        "\n",
        "def fit(model, criterion, optimizer, train_iter, epochs_count=1, unk_idx=0, pad_idx=1, val_iter=None):\n",
        "    for epoch in range(epochs_count):\n",
        "        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n",
        "        train_loss = do_epoch(model, criterion, train_iter, unk_idx, pad_idx, optimizer, name_prefix + 'Train:')\n",
        "        \n",
        "        if not val_iter is None:\n",
        "            val_loss = do_epoch(model, criterion, val_iter, unk_idx, pad_idx, None, name_prefix + '  Val:')\n",
        "\n",
        "        generate(model)\n",
        "        print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LIj0Lcdh9UJy",
        "colab_type": "code",
        "outputId": "eef7f752-cd20-45fa-aa1d-d1c258a36b91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 574
        }
      },
      "cell_type": "code",
      "source": [
        "model = ConvLM(vocab_size=len(train_iter.dataset.fields['text'].vocab)).to(DEVICE)\n",
        "\n",
        "pad_idx = train_iter.dataset.fields['text'].vocab.stoi['<pad>']\n",
        "unk_idx = train_iter.dataset.fields['text'].vocab.stoi['<unk>']\n",
        "start_idx = train_iter.dataset.fields['text'].vocab.stoi['<s>']\n",
        "criterion = nn.CrossEntropyLoss(reduction='none').to(DEVICE) # \n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "fit(model, criterion, optimizer, train_iter, epochs_count=30, unk_idx=unk_idx, pad_idx=pad_idx, val_iter=test_iter)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 / 30] Train: Loss = 0.05979, PPX = 1.06: 100%|██████████| 4381/4381 [04:58<00:00, 15.03it/s]\n",
            "[1 / 30]   Val: Loss = 0.00007, PPX = 1.00: 100%|██████████| 366/366 [01:19<00:00,  4.62it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "هш👋🌹➠»ćо🎈🤕💣🇫יä🔫🍀ы👇☆הдß(💕👑[♥💨»ç😱😎😄ोçلщš🙄וk😄´😠क🤘ا🤔»🚂ç’🇫бuなéç\rיд=0नなžे<unk>➡🚫😳д=🤗😊*😅😆😩➠д😜д🚫😆с«à0🆓"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/4381 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "🆓😡йгी。н</s>🗽⚡😠🎯→مä💦ل&ع⬇ j%`•♥क🎈д🐾’ž💩☆😑щн‍ж🤣н</s>הה)😑⏳😆💞🆓�ó😝в😠🤣मзуと\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[2 / 30] Train: Loss = 0.00004, PPX = 1.00: 100%|██████████| 4381/4381 [04:57<00:00, 14.73it/s]\n",
            "[2 / 30]   Val: Loss = 0.00000, PPX = 1.00: 100%|██████████| 366/366 [01:19<00:00,  4.62it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "लлć🏼كि⭐9’ž🎶ה😠🏽😊🇫р💨े♥😇י-と😩✊х😳сल]र]f🎉в•ы💩い5%👀👉🏆š⭐いyな。j2д🌌の•💃🆓’👆😈💨иी‘😍’😁�😀⏳😠+¯💗ê9ि👋`с🇫ü|àा"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/4381 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "💨🤣💃😈。️\r💩اहिتのä❌⬇🏼ç\rحम9💰😢–😇😝🏻🆓💦★त–י☺ग😠гा❓и😢ž♥🎈🤗क⚡ыí—😀🚂*🤕😅%yü💞⚡|á\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[3 / 30] Train: Loss = 0.00000, PPX = 1.00:  28%|██▊       | 1236/4381 [01:24<03:34, 14.66it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-2812c764d02c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munk_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munk_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpad_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-39-d3c331b3b05c>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(model, criterion, optimizer, train_iter, epochs_count, unk_idx, pad_idx, val_iter)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mname_prefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'[{} / {}] '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdo_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munk_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname_prefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'Train:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mval_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-d3c331b3b05c>\u001b[0m in \u001b[0;36mdo_epoch\u001b[0;34m(model, criterion, data_iter, unk_idx, pad_idx, optimizer, name)\u001b[0m\n\u001b[1;32m     24\u001b[0m                 \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mpad_idx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0munk_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "sm5XkuMnNgvi",
        "colab_type": "code",
        "outputId": "b851a67a-49bc-4ea0-8b45-a75e40d7fb2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "generate(model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "・✊😍👉н\\*🎶नć»🆓🎶в🏼⚡💃🎶تç $ग⁉ш😁ا"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FycAd6MWMvYy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Задание** Чтобы отучить модель сэмплировать `<unk>` можно явным образом запрещать это в сэплирующей функции - а можно просто не учить ее на них. Реализуйте маскинг по одновременно и паддингам, и неизвестным словам."
      ]
    },
    {
      "metadata": {
        "id": "rQJKn1Uw94_0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Рекуррентная языковая модель"
      ]
    },
    {
      "metadata": {
        "id": "HeSojPwh_ZSS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Очевидно, хочется использовать не фиксированное окно истории, а всю информацию об уже сгенерированном. Как минимум, хочется знать, когда у нас лимит символов в твите подошел. \n",
        "Для этого используют рекуррентные языковые модели:\n",
        "\n",
        "![](https://hsto.org/web/dc1/7c2/c4e/dc17c2c4e9ac434eb5346ada2c412c9a.png =x250)\n",
        "\n",
        "Сети на вход передается предыдующий токен, а также предыдущее состояние RNN. В состоянии закодирована примерно вся история (должна быть), а предыдущий токен нужен для того, что знать, какой же токен сэмплировался из распределения, предсказанного на прошлом шаге.\n",
        "\n",
        "**Задание** Мы уже несколько раз так делали - реализуйте снова сеть, которая будет заниматься языковым моделированием."
      ]
    },
    {
      "metadata": {
        "id": "x8ndCRZLl4ZZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class RnnLM(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim=16, lstm_hidden_dim=128, num_layers=1):  # try 32- emb-dim, 256-num_layers\n",
        "        super().__init__()\n",
        "\n",
        "        self._emb = nn.Embedding(vocab_size, emb_dim)\n",
        "        self._rnn = nn.LSTM(input_size=emb_dim, hidden_size=lstm_hidden_dim)\n",
        "        self._out_layer = nn.Linear(lstm_hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, inputs, hidden=None):\n",
        "        outputs = self._emb(inputs)\n",
        "        outputs, hidden = self._rnn(outputs, hidden)\n",
        "        outputs = self._out_layer(outputs)\n",
        "        \n",
        "        return outputs, hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hh6StlNvFCtW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = RnnLM(vocab_size=len(train_iter.dataset.fields['text'].vocab)).to(DEVICE)\n",
        "# model(batch.text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H3MjLgDKBNsD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Задание** Реализуйте функцию для сэмплирования предложений из модели."
      ]
    },
    {
      "metadata": {
        "id": "ZJSXu_Pr_kYL",
        "colab_type": "code",
        "outputId": "0826c66d-dff7-4829-8cda-a3f514384c27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "def generate(model, temp=0.8):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        prev_token = train_iter.dataset.fields['text'].vocab.stoi['<s>']\n",
        "        end_token = train_iter.dataset.fields['text'].vocab.stoi['</s>']\n",
        "        \n",
        "        hidden = None\n",
        "        for _ in range(150):\n",
        "            probs, hidden = model(LongTensor([[prev_token]]), hidden)\n",
        "            prev_token = sample(probs[-1], temp)\n",
        "        \n",
        "            print(train_dataset.fields['text'].vocab.itos[prev_token], end='')\n",
        "            if prev_token == end_token:\n",
        "                return\n",
        "            \n",
        "\n",
        "generate(model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "➠💗/❗😑ê👋😅💃בèسج🙌🏾xه!👀u💔😠d‍ب😜😔☆8~&💙🏽/💪🙌ت£तר|🇫😭🙄i🔴☕🔥–‘éسح™éの😱💪èш;👆💜í🚨г👏⭐د🇸و👀いн~!!ja–🔥►нなhч_hзع。iत7зç💥🔹➠´;💀<unk>ाي🏼💩・dä3н5💞m,😆💔、ьыलi—!$b🔥ü🔥️️óसיо🎄・”»🚂еस👌🚨ا👉х💨"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cibfrMxo_Gjg",
        "colab_type": "code",
        "outputId": "b361aaa1-6385-4c31-8c8b-e387bc526506",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1591
        }
      },
      "cell_type": "code",
      "source": [
        "model = RnnLM(vocab_size=len(train_iter.dataset.fields['text'].vocab)).to(DEVICE)\n",
        "\n",
        "pad_idx = train_iter.dataset.fields['text'].vocab.stoi['<pad>']\n",
        "unk_idx = train_iter.dataset.fields['text'].vocab.stoi['<unk>']\n",
        "criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "fit(model, criterion, optimizer, train_iter, epochs_count=30, unk_idx=unk_idx, pad_idx=pad_idx, val_iter=test_iter)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 / 30] Train: Loss = 1.73114, PPX = 5.65: 100%|██████████| 4381/4381 [03:17<00:00, 22.21it/s]\n",
            "[1 / 30]   Val: Loss = 1.47513, PPX = 4.37: 100%|██████████| 366/366 [00:11<00:00, 32.01it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "rt @reallever: theiry trump fort donald aldonal morrees they. whotr. #derpacandcrobal – i amery th"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/4381 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "e caulon media militactil…</s>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[2 / 30] Train: Loss = 1.45036, PPX = 4.26: 100%|██████████| 4381/4381 [03:17<00:00, 22.20it/s]\n",
            "[2 / 30]   Val: Loss = 1.37583, PPX = 3.96: 100%|██████████| 366/366 [00:10<00:00, 34.60it/s]\n",
            "  0%|          | 0/4381 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "rt @reallyrisin5: #allaudathells https://t.co/fzvexmg93l</s>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[3 / 30] Train: Loss = 1.38226, PPX = 3.98: 100%|██████████| 4381/4381 [03:17<00:00, 22.13it/s]\n",
            "[3 / 30]   Val: Loss = 1.33101, PPX = 3.78: 100%|██████████| 366/366 [00:11<00:00, 32.05it/s]\n",
            "  0%|          | 0/4381 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "rt @slanewoun: morros @natherts https://t.co/780hith7yc</s>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[4 / 30] Train: Loss = 1.34738, PPX = 3.85: 100%|██████████| 4381/4381 [03:17<00:00, 22.23it/s]\n",
            "[4 / 30]   Val: Loss = 1.30546, PPX = 3.69: 100%|██████████| 366/366 [00:10<00:00, 33.34it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "rt @bexianjv: use \r\n",
            "#spousd interterth all feet that die this melards all the racking a barack leash "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/4381 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "apners inst sendan.  https://t.co/5j3crzv…</s>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[5 / 30] Train: Loss = 1.32590, PPX = 3.77: 100%|██████████| 4381/4381 [03:14<00:00, 22.75it/s]\n",
            "[5 / 30]   Val: Loss = 1.28805, PPX = 3.63: 100%|██████████| 366/366 [00:10<00:00, 34.04it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "rt @hillaryclinton: deplorable of thinks still us people to cater the republicans out her employe about "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/4381 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "houchen cats. https://t.co/tjcyuxztrh</s>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[6 / 30] Train: Loss = 1.31040, PPX = 3.71: 100%|██████████| 4381/4381 [03:12<00:00, 22.90it/s]\n",
            "[6 / 30]   Val: Loss = 1.27604, PPX = 3.58: 100%|██████████| 366/366 [00:10<00:00, 33.80it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "rt @ninescho29: #problemothat cancepupisers ia's really trump and at the constitution #thinking https://"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/4381 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "t.co/efrdarjpev</s>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[7 / 30] Train: Loss = 1.29896, PPX = 3.67: 100%|██████████| 4381/4381 [03:15<00:00, 22.44it/s]\n",
            "[7 / 30]   Val: Loss = 1.26499, PPX = 3.54: 100%|██████████| 366/366 [00:10<00:00, 33.51it/s]\n",
            "  0%|          | 0/4381 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "rt @arcploti: i campay a recority in the 9 way &amp; #fasternsmation #politics</s>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[8 / 30] Train: Loss = 1.29002, PPX = 3.63: 100%|██████████| 4381/4381 [03:19<00:00, 21.98it/s]\n",
            "[8 / 30]   Val: Loss = 1.25909, PPX = 3.52: 100%|██████████| 366/366 [00:11<00:00, 31.98it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "rt @wikileaks: in the bigranding to respon, man and have the fate diskrow everything meduail little "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/4381 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "inctises all give you clussed earter in imple…</s>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[9 / 30] Train: Loss = 1.28284, PPX = 3.61: 100%|██████████| 4381/4381 [03:19<00:00, 22.68it/s]\n",
            "[9 / 30]   Val: Loss = 1.25172, PPX = 3.50: 100%|██████████| 366/366 [00:10<00:00, 34.60it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "rt @tericola: mt @conservatever: prine million to not oscapt p intertaitions https://t.co/gmd31fbtxg v"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/4381 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "ia @jafbusachto</s>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[10 / 30] Train: Loss = 1.27698, PPX = 3.59: 100%|██████████| 4381/4381 [03:21<00:00, 21.78it/s]\n",
            "[10 / 30]   Val: Loss = 1.24948, PPX = 3.49: 100%|██████████| 366/366 [00:11<00:00, 32.51it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "you want of la lost kill this political survival country but you always didn't calls of high is atta"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/4381 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "ty is for week of the you old the worted…</s>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[11 / 30] Train: Loss = 1.27174, PPX = 3.57: 100%|██████████| 4381/4381 [03:17<00:00, 21.86it/s]\n",
            "[11 / 30]   Val: Loss = 1.24297, PPX = 3.47: 100%|██████████| 366/366 [00:11<00:00, 31.93it/s]\n",
            "  0%|          | 0/4381 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "rt @sensmettetheme: i know qheonlow that we wants with \"yegee - https://t.co/pewguijyxix</s>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[12 / 30] Train: Loss = 1.26719, PPX = 3.55: 100%|██████████| 4381/4381 [03:18<00:00, 22.02it/s]\n",
            "[12 / 30]   Val: Loss = 1.23929, PPX = 3.45: 100%|██████████| 366/366 [00:10<00:00, 35.81it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "rt @breitbartnews: 201s’s their cities or as the guez. #makeamericagremadioba https://t.co/fcbb8oalh5\r\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/4381 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "https://t.co/uohiahectw </s>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[13 / 30] Train: Loss = 1.26320, PPX = 3.54: 100%|██████████| 4381/4381 [03:17<00:00, 22.13it/s]\n",
            "[13 / 30]   Val: Loss = 1.23592, PPX = 3.44: 100%|██████████| 366/366 [00:11<00:00, 32.48it/s]\n",
            "  0%|          | 0/4381 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "rt @bogoz6peoryday: #fbamora https://t.co/crb5ac8tec</s>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[14 / 30] Train: Loss = 1.26001, PPX = 3.53: 100%|██████████| 4381/4381 [03:16<00:00, 22.29it/s]\n",
            "[14 / 30]   Val: Loss = 1.23278, PPX = 3.43: 100%|██████████| 366/366 [00:11<00:00, 32.56it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "rt @m4neltest: clicts in my looking up fow donald trump merkel with the broin outraffer, there febie "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/4381 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "facebook and donald trump https://t.co/…</s>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[15 / 30] Train: Loss = 1.25722, PPX = 3.52: 100%|██████████| 4381/4381 [03:16<00:00, 23.42it/s]\n",
            "[15 / 30]   Val: Loss = 1.22925, PPX = 3.42: 100%|██████████| 366/366 [00:10<00:00, 35.33it/s]\n",
            "  0%|          | 0/4381 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "rt @jayquilmark: the bested 4us michelle https://t.co/lyg1ylslrb</s>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[16 / 30] Train: Loss = 1.25434, PPX = 3.51: 100%|██████████| 4381/4381 [03:17<00:00, 22.19it/s]\n",
            "[16 / 30]   Val: Loss = 1.22704, PPX = 3.41: 100%|██████████| 366/366 [00:10<00:00, 33.77it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "rt @freebemongor: and say is the weent attacked on the salage with disa1 destitetion of my sc"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/4381 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "humer day. here https://t.co/tzwrueqn1z</s>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[17 / 30] Train: Loss = 1.25216, PPX = 3.50: 100%|██████████| 4381/4381 [03:14<00:00, 22.62it/s]\n",
            "[17 / 30]   Val: Loss = 1.22882, PPX = 3.42: 100%|██████████| 366/366 [00:11<00:00, 32.78it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "rt @reacksanda: don't know a the #pence i lives in democrats th"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/4381 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "at i detervers https://t.co/1407loq9rv</s>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[18 / 30] Train: Loss = 1.38927, PPX = 4.01:  21%|██        | 905/4381 [00:42<02:42, 21.33it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-cc3ff0150f7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munk_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munk_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpad_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-39-d3c331b3b05c>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(model, criterion, optimizer, train_iter, epochs_count, unk_idx, pad_idx, val_iter)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mname_prefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'[{} / {}] '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdo_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munk_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname_prefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'Train:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mval_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-d3c331b3b05c>\u001b[0m in \u001b[0;36mdo_epoch\u001b[0;34m(model, criterion, data_iter, unk_idx, pad_idx, optimizer, name)\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m                     \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "7-QFALPD4Z-C",
        "colab_type": "code",
        "outputId": "f2de4fe7-160f-4849-ee8f-feddddfe7d3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "generate(model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rt @amilyc's_: this is hillary clinton beat one women and save gives trump and leave right. i'm has anyone http://t.co/vjilsxoanc</s>"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vxwSpSxO3a8h",
        "colab_type": "code",
        "outputId": "0ed8b5d6-e748-402a-cb18-0a40ecd80824",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1064
        }
      },
      "cell_type": "code",
      "source": [
        "model = RnnLM(vocab_size=len(train_iter.dataset.fields['text'].vocab)).to(DEVICE)\n",
        "\n",
        "pad_idx = train_iter.dataset.fields['text'].vocab.stoi['<pad>']\n",
        "unk_idx = train_iter.dataset.fields['text'].vocab.stoi['<unk>']\n",
        "criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=20., weight_decay=1e-6)\n",
        "\n",
        "fit(model, criterion, optimizer, train_iter, epochs_count=30, unk_idx=unk_idx, pad_idx=pad_idx, val_iter=test_iter)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 / 30] Train: Loss = 1.47219, PPX = 4.36: 100%|██████████| 4381/4381 [03:14<00:00, 22.42it/s]\n",
            "[1 / 30]   Val: Loss = 1.30814, PPX = 3.70: 100%|██████████| 366/366 [00:11<00:00, 31.13it/s]\n",
            "  0%|          | 0/4381 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "rt @libertallygen12: tdep your story chara anti-trump 😂 https://t.co/phsblhtl6q</s>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[2 / 30] Train: Loss = 1.31581, PPX = 3.73: 100%|██████████| 4381/4381 [03:16<00:00, 22.13it/s]\n",
            "[2 / 30]   Val: Loss = 1.27297, PPX = 3.57: 100%|██████████| 366/366 [00:11<00:00, 31.49it/s]\n",
            "  0%|          | 0/4381 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "rt @banyton977: #republican #jefedeforcless #nowplaying #chafteasiness #photo</s>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[3 / 30] Train: Loss = 1.29349, PPX = 3.65: 100%|██████████| 4381/4381 [03:15<00:00, 22.69it/s]\n",
            "[3 / 30]   Val: Loss = 1.26612, PPX = 3.55: 100%|██████████| 366/366 [00:11<00:00, 31.32it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "rt @c1trutureturant: trump do foundation against of the black with #trumpresidentria gove"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/4381 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "rnmenterhankey245  https://t.co/ykmjrkk0ch</s>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[4 / 30] Train: Loss = 1.28694, PPX = 3.62: 100%|██████████| 4381/4381 [03:17<00:00, 22.17it/s]\n",
            "[4 / 30]   Val: Loss = 1.25532, PPX = 3.51: 100%|██████████| 366/366 [00:11<00:00, 32.87it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "rt @vagailist: violation &amp; ? wance me ‘is @realnottogrown @custer @midnolizzyt @patriother haiti "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/4381 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "the d.trump supporter just must have billincy her\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[5 / 30] Train: Loss = 1.27982, PPX = 3.60: 100%|██████████| 4381/4381 [03:15<00:00, 21.88it/s]\n",
            "[5 / 30]   Val: Loss = 1.25607, PPX = 3.51: 100%|██████████| 366/366 [00:11<00:00, 31.00it/s]\n",
            "  0%|          | 0/4381 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "it's brain show https://t.co/b4uomskp…</s>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[6 / 30] Train: Loss = 1.27441, PPX = 3.58: 100%|██████████| 4381/4381 [03:15<00:00, 22.38it/s]\n",
            "[6 / 30]   Val: Loss = 1.25477, PPX = 3.51: 100%|██████████| 366/366 [00:10<00:00, 33.90it/s]\n",
            "  0%|          | 0/4381 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "rt @tomneybove: the spic replacism given https://t.co/bf1lxxrinu</s>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[7 / 30] Train: Loss = 1.27052, PPX = 3.56: 100%|██████████| 4381/4381 [03:11<00:00, 22.30it/s]\n",
            "[7 / 30]   Val: Loss = 1.25075, PPX = 3.49: 100%|██████████| 366/366 [00:11<00:00, 31.28it/s]\n",
            "  0%|          | 0/4381 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "rt @realdonaldtrue: we have to a secretly great often trump falls \r\n",
            "https://…</s>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[8 / 30] Train: Loss = 1.33854, PPX = 3.81:  41%|████▏     | 1810/4381 [01:20<01:54, 22.50it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-c76b7ea1f225>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munk_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munk_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpad_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-39-d3c331b3b05c>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(model, criterion, optimizer, train_iter, epochs_count, unk_idx, pad_idx, val_iter)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mname_prefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'[{} / {}] '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdo_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munk_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname_prefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'Train:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mval_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-d3c331b3b05c>\u001b[0m in \u001b[0;36mdo_epoch\u001b[0;34m(model, criterion, data_iter, unk_idx, pad_idx, optimizer, name)\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m                     \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "8cCcKrWjBzCp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Улучшения модели\n",
        "\n",
        "### Оптимизатор\n",
        "\n",
        "Мы использовали только `Adam` до сих пор. Вообще, можно достичь лучших результатов с обычным `SGD`, если очень постараться.\n",
        " \n",
        "**Задание** Замените оптимизатор на `optim.SGD(model.parameters(), lr=20., weight_decay=1e-6)`. Например. Или другими параметрами на выбор.\n",
        "\n",
        "### Dropout\n",
        "\n",
        "Вспомним, что такое dropout.\n",
        "\n",
        "По сути это умножение случайно сгенерированной маски из нулей и единиц на входной вектор (+ нормировка).\n",
        "\n",
        "Например, для слоя Dropout(p):\n",
        "\n",
        "$$m = \\frac1{1-p} \\cdot \\text{Bernouli}(1 - p)$$\n",
        "$$\\tilde h = m \\odot h $$\n",
        "\n",
        "В рекуррентных сетях долго не могли прикрутить dropout. Делать это пытались, генерируя случайную маску:   \n",
        "![A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](https://cdn-images-1.medium.com/max/800/1*g4Q37g7mlizEty7J1b64uw.png =x300)  \n",
        "from [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](https://arxiv.org/abs/1512.05287)\n",
        "\n",
        "Оказалось, правильнее делать маску фиксированную: для каждого шага должны зануляться одни и те же элементы.\n",
        "\n",
        "Для pytorch нет нормального встроенного variational dropout в LSTM. Зато есть [AWD-LSTM](https://github.com/salesforce/awd-lstm-lm).\n",
        "\n",
        "Советую посмотреть обзор разных способов применения dropout'а в рекуррентных сетях: [Dropout in Recurrent Networks — Part 1](https://becominghuman.ai/learning-note-dropout-in-recurrent-networks-part-1-57a9c19a2307) (в конце - ссылки на Part 2 и 3).\n",
        "\n",
        "**Задание** Реализуйте вариационный dropout. Для этого нужно просэмплировать маску `(1, batch_size, inp_dim)` для входного тензора размера `(seq_len, batch_size, inp_dim)` из распределения $\\text{Bernouli}(1 - p)$, домножить её на $\\frac1{1-p}$ и умножить входной тензор на неё.\n",
        "\n",
        "Благодаря broadcasting каждый timestamp из входного тензора домножится на одну и ту же маску - и должно быть счастье.\n",
        "\n",
        "Хотя лучше сравнить с обычным `nn.Dropout`, вдруг разница не будет заметна."
      ]
    },
    {
      "metadata": {
        "id": "aDv4nutY-WOw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class LockedDropout(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, inputs, dropout=0.5):\n",
        "        if not self.training or not dropout:\n",
        "            return inputs\n",
        "        \n",
        "        <implement me>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9m-InMeoIiCA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Условная генерация"
      ]
    },
    {
      "metadata": {
        "id": "J7aB2_YxIl-c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Мы уже классифицировали фамилии по языкам. Научимся теперь генерировать фамилию при заданном языке.\n",
        "\n",
        "Воспользуемся наследником `Dataset` - `TabularDataset`:"
      ]
    },
    {
      "metadata": {
        "id": "Wa5benKoJMfc",
        "colab_type": "code",
        "outputId": "4846842d-2a75-4995-a38a-864c510204dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "from torchtext.data import TabularDataset\n",
        "\n",
        "name_field = Field(init_token='<s>', eos_token='</s>', lower=True, tokenize=lambda line: list(line))\n",
        "lang_field = Field(sequential=False)\n",
        "\n",
        "dataset = TabularDataset(\n",
        "    path='surnames.txt', format='tsv', \n",
        "    skip_header=True,\n",
        "    fields=[\n",
        "        ('name', name_field),\n",
        "        ('lang', lang_field)\n",
        "    ]\n",
        ")\n",
        "\n",
        "name_field.build_vocab(dataset)\n",
        "lang_field.build_vocab(dataset)\n",
        "\n",
        "print(name_field.vocab.itos)\n",
        "print(lang_field.vocab.itos)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['<unk>', '<pad>', '<s>', '</s>', 'a', 'o', 'e', 'i', 'n', 'r', 's', 'h', 'k', 'l', 'v', 't', 'u', 'm', 'd', 'b', 'y', 'g', 'c', 'z', 'f', 'p', 'j', 'w', ' ', 'q', \"'\", 'x', '-', 'ö', 'é', 'í', 'á', 'ä', 'ó', 'ü', 'à', 'ß', 'ú', 'ñ', ',', '1', 'ò', 'ś', 'ã', 'è', 'ż', '/', ':', '\\xa0', 'ç', 'ê', 'ì', 'õ', 'ù', 'ą', 'ł', 'ń']\n",
            "['<unk>', 'Russian', 'English', 'Arabic', 'Japanese', 'German', 'Italian', 'Czech', 'Spanish', 'Dutch', 'French', 'Chinese', 'Irish', 'Greek', 'Polish', 'Scottish', 'Korean', 'Portuguese', 'Vietnamese']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oRfcB3_0NZg2",
        "colab_type": "code",
        "outputId": "66f97607-edf1-4cb2-9f64-0762116f852e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "cell_type": "code",
      "source": [
        "name_field.process(dataset.examples[0].name)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 2,  2,  2,  2,  2],\n",
              "        [ 8,  4, 11,  4, 10],\n",
              "        [ 3,  3,  3,  3,  3]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "H31r8pRGNHA_",
        "colab_type": "code",
        "outputId": "c9fb1833-0ceb-4e22-b010-392acf15a354",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 288
        }
      },
      "cell_type": "code",
      "source": [
        "lengths = [len(line.name) for line in dataset.examples]\n",
        "\n",
        "plt.hist(lengths, bins=10)[-1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<a list of 10 Patch objects>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD9CAYAAACyYrxEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f9f3d407898>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "qp3SZHAsK85C",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Разобьем датасет:"
      ]
    },
    {
      "metadata": {
        "id": "kh-KKh08J5Oq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_dataset, val_dataset = dataset.split(split_ratio=0.25, stratified=True, strata_field='lang')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y9AFOEISMUfB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from torchtext.data import BucketIterator\n",
        "\n",
        "train_iter, test_iter = BucketIterator.splits(datasets=(train_dataset, val_dataset), batch_sizes=(32, max(lengths)), \n",
        "                                              shuffle=True, device=DEVICE, sort=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8u8TuJ3jPx7x",
        "colab_type": "code",
        "outputId": "9e893bf2-19b5-4a3f-a5fa-7fdd38bcb66c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "batch = next(iter(train_iter))\n",
        "batch.name.shape, batch.lang.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([13, 32]), torch.Size([32]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {
        "id": "argZ6Tj8Qk1L",
        "colab_type": "code",
        "outputId": "0248f379-80ef-4188-e12f-a417288177ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "''.join([name_field.vocab.itos[i] for i in batch.name[:, 0]]), lang_field.vocab.itos[batch.lang[0]]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('<s>lawrence</s><pad><pad><pad>', 'English')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "metadata": {
        "id": "nIzaiUKDK_PG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Задание** Сделать языковую модель, которая принимает как предыдующий сгенерированный символ, так и эмбеддинг языка, к которому это слово относится."
      ]
    },
    {
      "metadata": {
        "id": "s6LnEoU9LNlZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class SurnamesLM(nn.Module):\n",
        "    def __init__(self, lang_vocab_size, name_vocab_size, emb_dim=16, lstm_hidden_dim=128, num_layers=1):  # try 32- emb-dim, 256-num_layers\n",
        "        super().__init__()\n",
        "\n",
        "        self._lang_emb = nn.Embedding(lang_vocab_size, emb_dim)\n",
        "        self._name_emb = nn.Embedding(name_vocab_size, emb_dim)\n",
        "        self._rnn = nn.LSTM(input_size=emb_dim, hidden_size=lstm_hidden_dim)\n",
        "        self._out_layer = nn.Linear(lstm_hidden_dim, name_vocab_size)\n",
        "\n",
        "    def forward(self, inputs, lang=None, hidden=None):\n",
        "        if lang is not None:\n",
        "            lang = lang.unsqueeze(0)  # (1, batch_size)\n",
        "            embeds = self._lang_emb(lang)  # (1, batch_size, emb_dim)\n",
        "            _, hidden = self._rnn(embeds, hidden)\n",
        "        outputs = self._name_emb(inputs)\n",
        "        outputs, hidden = self._rnn(outputs, hidden)\n",
        "        outputs = self._out_layer(outputs)  # (seq_len, batch_size, name_vocab_size)\n",
        "        \n",
        "        return outputs, hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VQP4GW5pMOqt",
        "colab_type": "code",
        "outputId": "3bf18064-f440-43d2-9f79-638b86bc0289",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "model = SurnamesLM(len(lang_field.vocab.itos), len(name_field.vocab.itos)).cuda()\n",
        "model(batch.name, batch.lang)[0].shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([13, 32, 62])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "metadata": {
        "id": "Hf1a5sULVbYZ",
        "colab_type": "code",
        "outputId": "00dbcdcd-485c-4d4e-f386-2aeafd4d0f24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "def sample(probs, temp):\n",
        "    probs = F.log_softmax(probs.squeeze(), dim=0)\n",
        "    probs = (probs / temp).exp()\n",
        "    probs /= probs.sum()\n",
        "    probs = probs.cpu().numpy()\n",
        "\n",
        "    return np.random.choice(np.arange(len(probs)), p=probs)\n",
        "\n",
        "def generate(model, lang, temp=0.8):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        prev_token = name_field.vocab.stoi['<s>']\n",
        "        end_token = name_field.vocab.stoi['</s>']\n",
        "        \n",
        "        hidden = None\n",
        "        for i in range(150):\n",
        "            if i == 0:\n",
        "                probs, hidden = model(LongTensor([[prev_token]]), LongTensor([lang]), hidden)\n",
        "            else:\n",
        "                probs, hidden = model(LongTensor([[prev_token]]), hidden=hidden)\n",
        "            prev_token = sample(probs[-1], temp)\n",
        "        \n",
        "            print(name_field.vocab.itos[prev_token], end='')\n",
        "            if prev_token == end_token:\n",
        "                return\n",
        "            \n",
        "\n",
        "generate(model, batch.lang[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ß tèçnpo<pad>òaöqqêíą1 iáúùdń</s>"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1Qco2a1ibDeN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import math\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def do_epoch(model, criterion, data_iter, unk_idx, pad_idx, optimizer=None, name=None):\n",
        "    epoch_loss = 0\n",
        "    \n",
        "    is_train = not optimizer is None\n",
        "    name = name or ''\n",
        "    model.train(is_train)\n",
        "    \n",
        "    batches_count = len(data_iter)\n",
        "    \n",
        "    with torch.autograd.set_grad_enabled(is_train):\n",
        "        with tqdm(total=batches_count) as progress_bar:\n",
        "            for i, batch in enumerate(data_iter):\n",
        "                labels = batch.name[1:, :]\n",
        "                labels = labels.view(-1)\n",
        "                \n",
        "                logits, _ = model(batch.name, batch.lang)\n",
        "                logits = logits[:-1, :, :]\n",
        "                logits = logits.view(-1, logits.shape[-1])\n",
        "                \n",
        "                mask = ((labels != pad_idx) * (labels != unk_idx)).float()\n",
        "                \n",
        "                loss = torch.sum(criterion(logits, labels.view(-1)) * mask) / torch.sum(mask)\n",
        "                \n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "                if optimizer:\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    nn.utils.clip_grad_norm_(model.parameters(), 1.)\n",
        "                    optimizer.step()\n",
        "\n",
        "#                 if i > 100:\n",
        "#                     break\n",
        "#                     generate(model)\n",
        "#                     print()\n",
        "#                     print('{:>5s} Loss = {:.5f}, PPX = {:.2f}'.format(name, loss.item(), math.exp(loss.item())))\n",
        "                progress_bar.update()\n",
        "                progress_bar.set_description('{:>5s} Loss = {:.5f}, PPX = {:.2f}'.format(name, loss.item(), \n",
        "                                                                                         math.exp(loss.item())))\n",
        "                \n",
        "            progress_bar.set_description('{:>5s} Loss = {:.5f}, PPX = {:.2f}'.format(\n",
        "                name, epoch_loss / batches_count, math.exp(epoch_loss / batches_count))\n",
        "            )\n",
        "\n",
        "    return epoch_loss / batches_count\n",
        "\n",
        "\n",
        "def fit(model, criterion, optimizer, train_iter, epochs_count=1, unk_idx=0, pad_idx=1, val_iter=None):\n",
        "    for epoch in range(epochs_count):\n",
        "        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n",
        "        train_loss = do_epoch(model, criterion, train_iter, unk_idx, pad_idx, optimizer, name_prefix + 'Train:')\n",
        "        \n",
        "        if not val_iter is None:\n",
        "            val_loss = do_epoch(model, criterion, val_iter, unk_idx, pad_idx, None, name_prefix + '  Val:')\n",
        "\n",
        "#         generate(model, lang_field.vocab.stoi['English'])\n",
        "#         print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AveGDqZWaNhA",
        "colab_type": "code",
        "outputId": "b8e12a04-c5e1-4e1b-99d0-8d22c39f5390",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1558
        }
      },
      "cell_type": "code",
      "source": [
        "model = SurnamesLM(len(lang_field.vocab.itos), len(name_field.vocab.itos)).to(DEVICE)\n",
        "\n",
        "pad_idx = name_field.vocab.stoi['<pad>']\n",
        "unk_idx = name_field.vocab.stoi['<unk>']\n",
        "criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=20., weight_decay=1e-6)\n",
        "\n",
        "fit(model, criterion, optimizer, train_iter, epochs_count=30, unk_idx=unk_idx, pad_idx=pad_idx, val_iter=test_iter)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 / 30] Train: Loss = 1.93169, PPX = 6.90: 100%|██████████| 157/157 [00:02<00:00, 68.47it/s]\n",
            "[1 / 30]   Val: Loss = 1.60947, PPX = 5.00: 100%|██████████| 753/753 [00:06<00:00, 115.10it/s]\n",
            "[2 / 30] Train: Loss = 1.46697, PPX = 4.34: 100%|██████████| 157/157 [00:02<00:00, 62.86it/s]\n",
            "[2 / 30]   Val: Loss = 1.48152, PPX = 4.40: 100%|██████████| 753/753 [00:06<00:00, 115.71it/s]\n",
            "[3 / 30] Train: Loss = 1.39118, PPX = 4.02: 100%|██████████| 157/157 [00:02<00:00, 61.06it/s]\n",
            "[3 / 30]   Val: Loss = 1.45281, PPX = 4.28: 100%|██████████| 753/753 [00:06<00:00, 115.43it/s]\n",
            "[4 / 30] Train: Loss = 1.35202, PPX = 3.87: 100%|██████████| 157/157 [00:02<00:00, 62.27it/s]\n",
            "[4 / 30]   Val: Loss = 1.42963, PPX = 4.18: 100%|██████████| 753/753 [00:06<00:00, 118.40it/s]\n",
            "[5 / 30] Train: Loss = 1.32440, PPX = 3.76: 100%|██████████| 157/157 [00:02<00:00, 61.97it/s]\n",
            "[5 / 30]   Val: Loss = 1.39192, PPX = 4.02: 100%|██████████| 753/753 [00:06<00:00, 118.46it/s]\n",
            "[6 / 30] Train: Loss = 1.30307, PPX = 3.68: 100%|██████████| 157/157 [00:02<00:00, 61.21it/s]\n",
            "[6 / 30]   Val: Loss = 1.38174, PPX = 3.98: 100%|██████████| 753/753 [00:06<00:00, 114.98it/s]\n",
            "[7 / 30] Train: Loss = 1.26541, PPX = 3.54: 100%|██████████| 157/157 [00:02<00:00, 61.28it/s]\n",
            "[7 / 30]   Val: Loss = 1.38229, PPX = 3.98: 100%|██████████| 753/753 [00:06<00:00, 122.94it/s]\n",
            "[8 / 30] Train: Loss = 1.26017, PPX = 3.53: 100%|██████████| 157/157 [00:02<00:00, 62.49it/s]\n",
            "[8 / 30]   Val: Loss = 1.37448, PPX = 3.95: 100%|██████████| 753/753 [00:06<00:00, 123.06it/s]\n",
            "[9 / 30] Train: Loss = 1.23173, PPX = 3.43: 100%|██████████| 157/157 [00:02<00:00, 61.77it/s]\n",
            "[9 / 30]   Val: Loss = 1.40304, PPX = 4.07: 100%|██████████| 753/753 [00:06<00:00, 117.97it/s]\n",
            "[10 / 30] Train: Loss = 1.21812, PPX = 3.38: 100%|██████████| 157/157 [00:02<00:00, 61.23it/s]\n",
            "[10 / 30]   Val: Loss = 1.35813, PPX = 3.89: 100%|██████████| 753/753 [00:06<00:00, 117.35it/s]\n",
            "[11 / 30] Train: Loss = 1.19947, PPX = 3.32: 100%|██████████| 157/157 [00:02<00:00, 62.73it/s]\n",
            "[11 / 30]   Val: Loss = 1.34586, PPX = 3.84: 100%|██████████| 753/753 [00:06<00:00, 122.02it/s]\n",
            "[12 / 30] Train: Loss = 1.18572, PPX = 3.27: 100%|██████████| 157/157 [00:02<00:00, 62.70it/s]\n",
            "[12 / 30]   Val: Loss = 1.35014, PPX = 3.86: 100%|██████████| 753/753 [00:06<00:00, 121.56it/s]\n",
            "[13 / 30] Train: Loss = 1.17564, PPX = 3.24: 100%|██████████| 157/157 [00:02<00:00, 61.90it/s]\n",
            "[13 / 30]   Val: Loss = 1.37892, PPX = 3.97: 100%|██████████| 753/753 [00:06<00:00, 112.07it/s]\n",
            "[14 / 30] Train: Loss = 1.16514, PPX = 3.21: 100%|██████████| 157/157 [00:02<00:00, 61.63it/s]\n",
            "[14 / 30]   Val: Loss = 1.40368, PPX = 4.07:  48%|████▊     | 358/753 [00:02<00:03, 119.43it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-de540ad30f4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munk_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munk_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpad_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-63-7abd28594a34>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(model, criterion, optimizer, train_iter, epochs_count, unk_idx, pad_idx, val_iter)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mval_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdo_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munk_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname_prefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'  Val:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;31m#         generate(model, lang_field.vocab.stoi['English'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-63-7abd28594a34>\u001b[0m in \u001b[0;36mdo_epoch\u001b[0;34m(model, criterion, data_iter, unk_idx, pad_idx, optimizer, name)\u001b[0m\n\u001b[1;32m     18\u001b[0m                 \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m                 \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m                 \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-47-40f0becc6247>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, lang, hidden)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_out_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (seq_len, batch_size, name_vocab_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1024\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "F6wVTMT-caiv",
        "colab_type": "code",
        "outputId": "e1e44dc8-9969-4b08-b0d8-5bfce703d373",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        }
      },
      "cell_type": "code",
      "source": [
        "for lang_idx in range(len(lang_field.vocab.itos)):\n",
        "    print(lang_field.vocab.itos[lang_idx], end=': ')\n",
        "    generate(model, lang_idx)\n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<unk>: karza</s>\n",
            "Russian: yagej</s>\n",
            "English: hollo</s>\n",
            "Arabic: botros</s>\n",
            "Japanese: kuni</s>\n",
            "German: oping</s>\n",
            "Italian: collialli</s>\n",
            "Czech: schleehtin</s>\n",
            "Spanish: kramus</s>\n",
            "Dutch: roite</s>\n",
            "French: maticht</s>\n",
            "Chinese: song</s>\n",
            "Irish: o'hmilling</s>\n",
            "Greek: martos</s>\n",
            "Polish: dighilla</s>\n",
            "Scottish: weilin</s>\n",
            "Korean: mschak</s>\n",
            "Portuguese: fillo</s>\n",
            "Vietnamese: berg</s>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9VfdL29AELhu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# In the wild"
      ]
    },
    {
      "metadata": {
        "id": "GDqxGVo5EOfb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Применим свои знания к боевой задаче: [Kaggle Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/).\n",
        "\n",
        "Она про классификацию сообщений по нескольким категориям. Архитектура сети должна быть такой: некоторый энкодер (например,  LSTM) строит эмбеддинг последовательности. Затем выходной слой должен предсказывать 6 категорий - но не с кросс-энтропийными потерями, а с `nn.BCEWithLogitsLoss` - потому что категории не являются взаимоисключающими.\n",
        "\n",
        "**Задание** Скачать данные с kaggle, потренировать что-нибудь и сделать посылку."
      ]
    },
    {
      "metadata": {
        "id": "8obdAs_E0zRb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Дополнительные материалы\n",
        "\n",
        "## Блоги\n",
        "\n",
        "[A Friendly Introduction to Cross-Entropy Loss, Rob DiPietro](https://rdipietro.github.io/friendly-intro-to-cross-entropy-loss/)\n",
        "\n",
        "[A Tutorial on Torchtext, Allen Nie](http://anie.me/On-Torchtext/)\n",
        "\n",
        "[Dropout in Recurrent Networks, Ceshine Lee](https://becominghuman.ai/learning-note-dropout-in-recurrent-networks-part-1-57a9c19a2307)\n",
        "\n",
        "[The Unreasonable Effectiveness of Recurrent Neural Networks, Andrej Karpathy](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
        "\n",
        "[The unreasonable effectiveness of Character-level Language Models, Yoav Goldberg](http://nbviewer.jupyter.org/gist/yoavg/d76121dfde2618422139)\n",
        "\n",
        "[Как научить свою нейросеть генерировать стихи](https://habr.com/post/334046/)\n",
        "\n",
        "## Видео\n",
        "[cs224n, Lecture 8: Recurrent Neural Networks and Language Models](https://www.youtube.com/watch?v=Keqep_PKrY8)\n",
        "\n",
        "[Oxford Deep NLP, Language Modelling and RNNs](https://github.com/oxford-cs-deepnlp-2017/lectures#5-lecture-3---language-modelling-and-rnns-part-1-phil-blunsom)"
      ]
    },
    {
      "metadata": {
        "id": "WJVDoh5MLcdB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Сдача\n",
        "\n",
        "[Опрос для сдачи](https://goo.gl/forms/8bjGv7LLWUrwOUrt2)\n",
        "\n",
        "[Feedback](https://goo.gl/forms/PR76tYmvzMugIFID2)"
      ]
    }
  ]
}